{"cells":[{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","\n","##############Step 1: Extract \"brief hospital course\" from discharge summary#####################\n","\n","def extract_HC(dc_summary_path):\n","\n","  # Load the data\n","  dc_summary_raw = pd.read_csv(dc_summary_path)\n","\n","  # Set up the regular expression to extract hospital course from discharge summary\n","  # Of note these patterns would not caputre all hospital courses, and is indeed a convservative approach to ensure quality of data\n","  pattern1  = re.compile(\"Brief Hospital Course.*\\n*((?:\\n.*)+?)(Medications on Admission|___  on Admission|___ on Admission)\")\n","  pattern2  = re.compile(\"Brief Hospital Course.*\\n*((?:\\n.*)+?)Discharge Medications\")\n","  pattern3  = re.compile(\"(Brief Hospital Course|rief Hospital Course|HOSPITAL COURSE)\\\n","                        .*\\n*((?:\\n.*)+?)\\\n","                        (Medications on Admission|Discharge Medications|DISCHARGE MEDICATIONS|DISCHARGE DIAGNOSIS|Discharge Disposition|___ Disposition|CONDITION ON DISCHARGE|DISCHARGE INSTRUCTIONS)\")\n","  pattern4  = re.compile(\"(Mg-[12].|LACTATE-[12].|Epi-|Gap-|COUNT-|TRF-)___(.*\\n*((?:\\n.*)+?))(Medications on Admission)\")\n","\n","\n","  # Idea here is to try more convservaite pattern first, if not work, try less conservative pattern\n","  def split_note(note):\n","    if re.search(pattern1, note):\n","      return re.search(pattern1, note).group(1)\n","    else:\n","      if re.search(pattern2, note):\n","        return re.search(pattern2, note).group(1)\n","      else:\n","        if re.search(pattern3, note):\n","          return re.search(pattern3, note).group(2)\n","        else:\n","          if re.search(pattern4, note):\n","            return re.search(pattern4, note).group(2)\n","          else:\n","            return None\n","\n","  # Apply the function to dc_summary_raw to extract hospital course\n","  dc_summary_raw[\"hospital_course\"] = dc_summary_raw[\"text\"].apply(split_note)\n","\n","  # Drop those records that do not have hospital course captured with above regular expression patterns\n","  dc_summary = dc_summary_raw[[\"hadm_id\", \"hospital_course\"]].dropna()\n","\n","  # Get the number of words for each hospital course. Note that the current method is not accurate due to presense of special characters, but it's good enough for our purpose\n","  dc_summary[\"num_words\"] = dc_summary[\"hospital_course\"].apply(lambda x: len(x.split()))\n","\n","  # Remove the notes with less than 40 words\n","  dc_summary = dc_summary[dc_summary[\"num_words\"] > 40]\n","\n","  # Remove duplicate hospital courses (but keep the first one), as most of these notes represent low quality data\n","  dc_summary = dc_summary.drop_duplicates(subset=[\"hospital_course\"], keep=\"first\")\n","\n","  # Mean number of words in the hospital course is 378\n","  dc_summary[\"num_words\"].mean()\n","\n","  # only keep hadm_id and hospital_course\n","  dc_summary = dc_summary[[\"hadm_id\", \"hospital_course\"]]\n","\n","  return dc_summary\n","\n","##############Step 2: Map all DRG codes to MS-DRG v34.0#####################\n","# HCFA DRG is the MS-DRG code (https://github.com/MIT-LCP/mimic-code/issues/1561)\n","def map_drg(mimic_drg_path, drg_34_path, my_mapping_path):\n","\n","  drg = pd.read_csv(mimic_drg_path)\n","\n","  drg = drg[[\"hadm_id\", \"drg_code\", \"drg_type\", \"description\"]][drg[\"drg_type\"] == \"HCFA\"]\n","\n","  # We mapped all MS-DRG codes to v.34, which was released in 2016\n","  # Read the DRG v.34.0 codes from the csv file\n","  drg_34 = pd.read_csv(drg_34_path, sep=\"\\t\", header=0)\n","\n","  # Extra the set of all DRG description mentioned in the dataset\n","  drg_mapping = pd.DataFrame(drg[\"description\"].drop_duplicates())\n","\n","  # Create a second column called tranformation, which is to make basic normalizaiton of the DRG descriptions (e.g., W to WITH, W/O to WITHOUT, & to AND)\n","  drg_mapping[\"transformation\"] = drg_mapping[\"description\"].str.replace(\"W/O\", \"WITHOUT\").str.replace(\" W \", \" WITH \").str.replace(\"&\", \"AND\")\n","  drg_mapping[\"transformation\"] = drg_mapping[\"transformation\"].str.replace(\",\", \"\").str.replace(\" CATH \", \" CATHETERIZATION \").str.replace(\" PROC \", \" PROCEDURES \")\n","\n","  # Read the mapping rule to MS-DRG v.34\n","  my_mapping = pd.read_csv(my_mapping_path, header=0)\n","\n","  # create a dictionay from my_mapping, where raw_description is the key and DRG_34_description is the value\n","  my_mapping_dict = dict(zip(my_mapping.raw_description, my_mapping.DRG_34_description))\n","\n","  # Create a thrid column called drg_34_description, which copy the transformation column if the description is in drg_34\n","  # otherwide copy the value from my_mapping_dict where the key is the transformation\n","  drg_mapping[\"drg_34_description\"] = drg_mapping[\"transformation\"].where(drg_mapping[\"transformation\"].isin(drg_34.Description), other=drg_mapping[\"transformation\"].map(my_mapping_dict))\n","\n","  # check number of na in drg_34_description: 20\n","  drg_mapping.drg_34_description.isna().sum()\n","\n","  #rename the column name of description to raw_description\n","  drg_mapping = drg_mapping.rename(columns={\"description\": \"raw_description\"})\n","\n","  # Crate a table called drg_code_mapping by joining drg_mapping and drg_34 by drg_34_description\n","  drg_code_mapping = pd.merge(drg_mapping, drg_34, how=\"left\", left_on=\"drg_34_description\", right_on=\"Description\")\n","\n","  # make a dictinoary from drg_code_mapping, where raw_description is the key and DRG is the value\n","  drg_mapping_dict = dict(zip(drg_code_mapping.raw_description, drg_code_mapping.DRG))\n","\n","  # In the table drg, create a new column called drg_34_description, which is the mapped value from drg_mapping_dict where the key is description\n","  drg[\"drg_34_code\"] = drg[\"description\"].map(drg_mapping_dict)\n","\n","  # drop the rows with na in drg_34_code\n","  drg = drg.dropna(subset=[\"drg_34_code\"])\n","\n","  # only keep hadm_id and drg_34_code, and change drg_34_code to int\n","  drg = drg[[\"hadm_id\", \"drg_34_code\"]].astype({\"drg_34_code\": int})\n","\n","  return drg\n","\n","################Step 3. Merge discharge summary and drg, and split into training/testing sets#####################\n","#merge drg and dc_summary by hadm_id\n","def merge_HC_drg(dc_summary, drg):\n","\n","  dc_drg = pd.merge(dc_summary, drg, how=\"inner\", on=\"hadm_id\")\n","\n","  # remove drg_34_code with less than 2 observations\n","  # in this step removed code 998, 985 and 793\n","  dc_drg = dc_drg.groupby(\"drg_34_code\").filter(lambda x: len(x) >= 2)\n","\n","\n","  # number of unique drg_34_code in dc_drg: 738\n","  drg_count = dc_drg.drg_34_code.nunique()\n","\n","\n","  # rank dc_drg by drg_34_code\n","  dc_drg = dc_drg.sort_values(by=[\"drg_34_code\"])\n","\n","  # make a new dataframe called id2label, where the first column is drg_34_code and the second column is the rank of drg_34_code starting form 0 to 737\n","  id2label = pd.DataFrame(dc_drg.drg_34_code.drop_duplicates())\n","  id2label[\"label\"] = range(0, drg_count)\n","\n","  # in dc_drg, create a new column called label, which is the mapped value from id2label where the key is drg_34_code\n","  dc_drg[\"label\"] = dc_drg[\"drg_34_code\"].map(dict(zip(id2label.drg_34_code, id2label.label)))\n","\n","  # split dc_drc into train and test, test takes 10% of the data, set radoom state to 42, stratify by label\n","  train, test = train_test_split(dc_drg, test_size=0.1, random_state=42, stratify=dc_drg.label)\n","\n","  # rename hospital_course to text, remove column of hadm_id and drg_34_code, and save train and test to csv\n","  train = train.rename(columns={\"hospital_course\": \"text\"})\n","  test = test.rename(columns={\"hospital_course\": \"text\"})\n","  train = train[[\"text\", \"label\"]]\n","  test = test[[\"text\", \"label\"]]\n","\n","  return train, test, id2label\n","\n","if __name__ == \"__main__\":\n","    # Read path from the json file\n","  with open('paths.json', 'r') as f:\n","      path = json.load(f)\n","      dc_summary_path = path[\"dc_summary_path\"]\n","      mimic_drg_path = path[\"mimic_drg_path\"]\n","      drg_34_path = path[\"drg_34_path\"]\n","      my_mapping_path = path[\"my_mapping_path\"]\n","      train_set_path = path[\"train_set_path\"]\n","      test_set_path = path[\"test_set_path\"]\n","      id2label_path = path[\"id2label_path\"]\n","\n","\n","  drg = map_drg(mimic_drg_path, drg_34_path, my_mapping_path)\n","  dc_summary = extract_HC(dc_summary_path)\n","\n","  train, test, id2label = merge_HC_drg(dc_summary, drg)\n","\n","  id2label.to_csv(id2label_path, index=False)\n","  train.to_csv(train_set_path, index=False)\n","  test.to_csv(test_set_path, index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"-gd3ng-DKTGS","executionInfo":{"status":"error","timestamp":1718091659161,"user_tz":-360,"elapsed":399,"user":{"displayName":"Rafsan Jany","userId":"08721334090885411304"}},"outputId":"d521529b-4e46-475a-9a94-a6dce46a5ab5"},"id":"-gd3ng-DKTGS","execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'drgcodes.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9653653e04af>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   \u001b[0mdrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_drg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmimic_drg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrg_34_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_mapping_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m   \u001b[0mdc_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_HC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdc_summary_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-9653653e04af>\u001b[0m in \u001b[0;36mmap_drg\u001b[0;34m(mimic_drg_path, drg_34_path, my_mapping_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmap_drg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmimic_drg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrg_34_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_mapping_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   \u001b[0mdrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmimic_drg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mdrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hadm_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"drg_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"drg_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"drg_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"HCFA\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drgcodes.csv'"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import json\n","\n","\n","\n","def drg_dissection(drg_34_path, train_set_path, test_set_path, id2label_path):\n","\n","  drg_34_dissection = pd.read_csv(drg_34_path, sep=\"\\t\", header=0)\n","\n","  # only keep the column on DRG and Description\n","  drg_34_dissection = drg_34_dissection[[\"DRG\", \"Description\"]]\n","\n","  # Make a new column called CC/MCC, where the value is 1 if the value in column Description contains \"WITH CC\" or \"W CC\"\n","  #  2 if the value in column Description contains \"WITH MCC\" or \"W MCC\", 0 if the value in column Description contains \"WITHOUT CC/MCC\" or \"W/O CC/MCC\"\n","  # 3 if the value in column Description contains \"WITHOUT MCC\"or \"W/O MCC\", else 4 (which essentially represents not applicable)\n","    # Note, in the approach here, with cc/mcc will be classified with cc\n","\n","  drg_34_dissection[\"CC/MCC\"] = drg_34_dissection[\"Description\"].apply(lambda x: 1 if (\"WITH CC\" in x) or (\"W CC\" in x) else\n","                                                                        (2 if (\"WITH MCC\" in x) or (\"W MCC\" in x) else\n","                                                                          (0 if (\"WITHOUT CC/MCC\" in x) or (\"W/O CC/MCC\" in x) else\n","                                                                            (3 if (\"WITHOUT MCC\" in x) or (\"W/O MCC\" in x) else 4))))\n","\n","\n","\n","  # Make a new collumn called principal_diagnosis, which uses regex to extraxt text before one of the following words:\n","  # \"WITH CC\", \"W CC\", \"WITH MCC\", \"W MCC\", \"WITHOUT CC/MCC\", \"W/O CC/MCC\", \"WITHOUT MCC\", \"W/O MCC\"\n","\n","  pc_patterh = r\"^(.*?)(?:WITH CC|W CC|WITH MCC|W MCC|WITHOUT CC/MCC|W/O CC/MCC|WITHOUT MCC|W/O MCC)\"\n","  def find_pc(note):\n","    if re.search(pc_patterh, note):\n","      return re.search(pc_patterh, note).group(1)\n","    else:\n","      return note\n","\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"Description\"].apply(find_pc)\n","\n","  #In the column of principal_diagnosis, clean up some typo and inconsistence in the offical DRG 34 table\n","\n","  pattern_proc = '|'.join([\"PROEDURESC \", \"PROEDURESC \", \"PROCEDURSE \", \"PROC \", \"PROCEDURE \"])\n","\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(pattern_proc,\"PROCEDURES \", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"BILIARY TRACT PROCEDURES EXCEPT ONLY CHOLECYST \", \"BILIARY TRACT PROCEDURES EXCEPT ONLY CHOLECYSTECTOMY \", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"CATHETERATION\",\"CATHETERIZATION\", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"GASTROENTERISTIS\",\"GASTROENTERITIS\", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"FIXATIOM\",\"FIXATION\", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"CHEMOTHERPY\",\"CHEMOTHERAPY\", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"REMOVAL INTERNAL\",\"REMOVAL OF INTERNAL\", regex=True)\n","  drg_34_dissection[\"principal_diagnosis\"] = drg_34_dissection[\"principal_diagnosis\"].str.replace(\"CHEMOTHERAPY WITH ACUTE LEUKEMIA AS SDX OR WITH HIGH DOSE CHEMOTHERAPY AGENT\",\"CHEMOTHERAPY WITH ACUTE LEUKEMIA AS SDX\", regex=True)\n","\n","\n","\n","  # Number of unique principal_diagnosis: 340\n","  PC_count = drg_34_dissection.principal_diagnosis.nunique()\n","  CC_count = 5\n","\n","  # Make a new collum called principal_diagnosis_lable, which is an interger from 0 to 340\n","  drg_34_dissection[\"principal_diagnosis_lable\"] = drg_34_dissection[\"principal_diagnosis\"].map(dict(zip(drg_34_dissection.principal_diagnosis.drop_duplicates(), range(0, PC_count))))\n","\n","  def to_categorical(y, num_classes):\n","      \"\"\" 1-hot encodes a tensor \"\"\"\n","      return np.eye(num_classes, dtype='uint8')[y]\n","\n","  # Make a new collum called multi_label, which is a one hot vector with 345 elements\n","  # The first 340 (0-339) elements are the one hot vector for principal_diagnosis_lable\n","  # The 340th to 344th (340-344) elements are the one hot vector for CC/MCC\n","  drg_34_dissection[\"multi_label\"] = drg_34_dissection.apply(lambda x: np.concatenate((to_categorical(x[\"principal_diagnosis_lable\"], num_classes=PC_count), to_categorical(x[\"CC/MCC\"], num_classes=CC_count))), axis=1)\n","\n","  # make a new collum called two_label, which is a list of two elements: principal_diagnosis_lable and CC/MCC\n","  drg_34_dissection[\"two_label\"] = drg_34_dissection.apply(lambda x: [x[\"principal_diagnosis_lable\"], x[\"CC/MCC\"]], axis=1)\n","\n","  ##make database\n","  train = pd.read_csv(train_set_path)\n","  test = pd.read_csv(test_set_path)\n","\n","  # read id to label mapping\n","  id2label = pd.read_csv(id2label_path)\n","\n","  # in train and test, add corresponding drg_34_code where label match the label in id2label\n","  train = train.merge(id2label, on=\"label\", how=\"left\")\n","  test = test.merge(id2label, on=\"label\", how=\"left\")\n","\n","  # in train and test, add column of multi_label where drg_34_code match the drg_34_code in drg_34_dissection\n","  train = train.merge(drg_34_dissection[[\"DRG\", \"multi_label\"]], left_on=\"drg_34_code\", right_on=\"DRG\", how=\"left\")\n","  test = test.merge(drg_34_dissection[[\"DRG\", \"multi_label\"]], left_on=\"drg_34_code\", right_on=\"DRG\", how=\"left\")\n","\n","\n","  # for train and test, only keep text and multi_label and rename multi_label to label\n","  train = train[[\"text\", \"multi_label\"]]\n","  train = train.rename(columns={\"multi_label\": \"label\"})\n","  test = test[[\"text\", \"multi_label\"]]\n","  test = test.rename(columns={\"multi_label\": \"label\"})\n","\n","  return train, test, drg_34_dissection\n","\n","if __name__ == \"__main__\":\n","    # Read path from the json file\n","  with open('paths.json', 'r') as f:\n","      path = json.load(f)\n","      drg_34_path = path[\"drg_34_path\"]\n","      drg_34_dissection_path = path[\"drg_34_dissection_path\"]\n","      train_set_path = path[\"train_set_path\"]\n","      test_set_path = path[\"test_set_path\"]\n","      id2label_path = path[\"id2label_path\"]\n","      multi_train_set_path = path[\"multi_train_set_path\"]\n","      multi_test_set_path = path[\"multi_test_set_path\"]\n","\n","  train, test, drg_34_dissection = drg_dissection(drg_34_path, train_set_path, test_set_path, id2label_path)\n","\n","  train.to_csv(multi_train_set_path, index=False)\n","  test.to_csv(multi_test_set_path, index=False)\n","  drg_34_dissection.to_csv(drg_34_dissection_path, index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"sEbKCkn4KS82","executionInfo":{"status":"error","timestamp":1718091650662,"user_tz":-360,"elapsed":413,"user":{"displayName":"Rafsan Jany","userId":"08721334090885411304"}},"outputId":"5e77f303-1824-499f-f90d-36afb26cb851"},"id":"sEbKCkn4KS82","execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'new_train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0816a3739911>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmulti_test_set_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"multi_test_set_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrg_34_dissection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrg_dissection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrg_34_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2label_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_train_set_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-0816a3739911>\u001b[0m in \u001b[0;36mdrg_dissection\u001b[0;34m(drg_34_path, train_set_path, test_set_path, id2label_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;31m##make database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m   \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_train.csv'"]}]},{"cell_type":"markdown","id":"d9794339","metadata":{"id":"d9794339"},"source":["# Combined Training Scripts"]},{"cell_type":"code","execution_count":null,"id":"d41faf97","metadata":{"id":"d41faf97"},"outputs":[],"source":["import os\n","from typing import List\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","import fire\n","import torch\n","from datasets import load_dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import json\n","\n","from huggingface_hub import notebook_login\n","import numpy as np\n","\n","from utils.eval_utils import cls_metrics\n","from utils.gen_utils import create_folder\n","\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        train_set_path = path[\"train_set_path\"]\n","        test_set_path = path[\"test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","\n","def train(\n","    base_model: str = \"emilyalsentzer/Bio_ClinicalBERT\",  # the only required argument\n","    train_data_path: str = train_set_path,\n","    val_data_path: str = test_set_path,\n","    cache_dir: str = catche_path,\n","    split: int = 100,\n","    micro_batch_size: int = 16, # based on the previous recommended practice for classification-oriented fine-tuning of BERT (Devlin et al. 2018; Adhikari et al. 2019)\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512, # consider changing to 1024\n","    model_name: str = \"bert\",\n","    wandb_project: str = \"classification\", #other options: \"generative\", \"multilabel-classification\",\n","    wandb_watch: str = \"gradients\",  # options: false | gradients | all ; issues when using all: I have since bypassed this issue by only logging gradient and instead of all.\n","    wandb_log_model: str = \"\",  # options: false | true\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_name}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels = pd.read_csv(train_data_path).label.nunique()\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        cache_dir=cache_dir)\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","\n","    def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            all_param += param.numel()\n","            if param.requires_grad:\n","                trainable_params += param.numel()\n","        print(\n","            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","        )\n","\n","    print_trainable_parameters(model)\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = load_dataset(\"csv\", data_files=train_data_path, split=f'train[:{split}%]')\n","    test_data = load_dataset(\"csv\", data_files=val_data_path, split=f'train[:{split}%]')\n","\n","    train_data= train_data.shard(num_shards=5000, index=0)\n","    test_data= test_data.shard(num_shards=2000, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True)\n","    tokenized_test = test_data.map(preprocess_function, batched=True)\n","\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics(predictions, labels, class_num=num_labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"bbf01751","metadata":{"id":"bbf01751"},"outputs":[],"source":["# Adopted framework from: https://github.com/tloen/alpaca-lora\n","\n","\n","import os\n","import pandas as pd\n","from datetime import datetime\n","import json\n","from typing import List\n","\n","import fire\n","import torch\n","from datasets import load_dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import LlamaTokenizer, LlamaForSequenceClassification\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n","    TaskType\n",")\n","import torch\n","\n","from utils.eval_utils import cls_metrics\n","from utils.gen_utils import create_folder\n","\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        train_set_path = path[\"train_set_path\"]\n","        test_set_path = path[\"test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","\n","def train(\n","    # model/data params\n","    base_model: str = \"decapoda-research/llama-7b-hf\",  # the only required argument\n","    model_size: str = \"7b\",\n","    train_data_path: str = train_set_path,\n","    val_data_path: str = test_set_path,\n","    split: int = 100,\n","    cache_dir: str = catche_path,\n","    micro_batch_size: int = 4,\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512,\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"score\"\n","    ],\n","    padding_side: str = \"right\",\n","    wandb_project: str = \"classification\",\n","    wandb_watch: str = \"gradients\",\n","    wandb_log_model: str = \"\",\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_size}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{padding_side}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels = pd.read_csv(train_data_path).label.nunique()\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"model_size: {model_size}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"lora_r: {lora_r}\\n\"\n","            f\"lora_alpha: {lora_alpha}\\n\"\n","            f\"lora_dropout: {lora_dropout}\\n\"\n","            f\"lora_target_modules: {lora_target_modules}\\n\"\n","            f\"padding_side: {padding_side}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    model = LlamaForSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        load_in_8bit=True,\n","        torch_dtype=torch.float16,\n","        device_map=device_map,\n","        cache_dir=cache_dir)\n","\n","    tokenizer = LlamaTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","    # This is to fix the bad token in \"decapoda-research/llama-7b-hf\"\n","\n","    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n","    model.config.bos_token_id = 1\n","    model.config.eos_token_id = 2\n","\n","    model = prepare_model_for_int8_training(model)\n","\n","    # note when passing task type as string argument, it will lead to error. May consider adding module_to_save manually\n","    config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        target_modules=lora_target_modules,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=TaskType.SEQ_CLS,\n","        modules_to_save=None,\n","    )\n","    model = get_peft_model(model, config)\n","\n","    if resume_from_checkpoint:\n","        # Check the available weights and load them\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"pytorch_model.bin\"\n","        )  # Full checkpoint\n","        if not os.path.exists(checkpoint_name):\n","            checkpoint_name = os.path.join(\n","                resume_from_checkpoint, \"adapter_model.bin\"\n","            )  # only LoRA model - LoRA config above has to fit\n","            resume_from_checkpoint = (\n","                False  # So the trainer won't try loading its state\n","            )\n","        # The two files above have a different name depending on how they were saved, but are actually the same.\n","        if os.path.exists(checkpoint_name):\n","            print(f\"Restarting from {checkpoint_name}\")\n","            adapters_weights = torch.load(checkpoint_name)\n","            set_peft_model_state_dict(model, adapters_weights)\n","        else:\n","            print(f\"Checkpoint {checkpoint_name} not found\")\n","\n","    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = load_dataset(\"csv\", data_files=train_data_path, split=f'train[:{split}%]')\n","    test_data = load_dataset(\"csv\", data_files=val_data_path, split=f'train[:{split}%]')\n","\n","    # train_data= train_data.shard(num_shards=20000, index=0)\n","    # test_data= test_data.shard(num_shards=500, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","    tokenized_test = test_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics(predictions, labels, class_num=num_labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        fp16=True,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        remove_unused_columns=False,\n","        label_names=[\"labels\"],\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","\n","if __name__ == \"__main__\":\n","\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"4277ef28","metadata":{"id":"4277ef28"},"outputs":[],"source":["import os\n","from typing import List\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","import fire\n","import torch\n","from datasets import load_dataset, Dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import LlamaTokenizer, LlamaForSequenceClassification\n","\n","from torch.nn import CrossEntropyLoss\n","from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n","from utils.eval_utils import cls_metrics_multi\n","\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n","    TaskType\n",")\n","import torch\n","import json\n","import numpy as np\n","\n","from utils.gen_utils import create_folder\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        multi_train_set_path = path[\"multi_train_set_path\"]\n","        multi_test_set_path = path[\"multi_test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","        drg_34_dissection_path = path[\"drg_34_dissection_path\"]\n","\n","def train(\n","    base_model: str = \"decapoda-research/llama-7b-hf\",  # the only required argument\n","    model_size: str = \"7b\",\n","    train_data_path: str = multi_train_set_path,\n","    val_data_path: str = multi_test_set_path,\n","    drg_mapping_path: str = drg_34_dissection_path,\n","    cache_dir: str = catche_path,\n","    split: int = 100,\n","    micro_batch_size: int = 4,\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512, # consider changing to 1024\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"score\"\n","    ],\n","    padding_side: str = \"right\",\n","    wandb_project: str = \"multilabel-classification\", #other options: \"generative\", \"multilabel-classification\",\n","    wandb_watch: str = \"gradients\",  # options: false | gradients | all ; issues when using all: I have since bypassed this issue by only logging gradient and instead of all.\n","    wandb_log_model: str = \"\",  # options: false | true\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_size}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{padding_side}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels_pc = pd.read_csv(drg_mapping_path).principal_diagnosis_lable.nunique()\n","    num_labels_cc = pd.read_csv(drg_mapping_path)[\"CC/MCC\"].nunique()\n","    num_labels = num_labels_pc + num_labels_cc\n","\n","    train_data = pd.read_csv(train_data_path, converters={\"label\": lambda x: np.fromstring(x[1:-1], dtype=float, sep=\" \")})\n","    test_data = pd.read_csv(val_data_path, converters={\"label\": lambda x: np.fromstring(x[1:-1], dtype=float, sep=\" \")})\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"model_size: {model_size}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels_pc: {num_labels_pc}\\n\"\n","            f\"num_labels_cc: {num_labels_cc}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"num_train_data: {len(train_data)}\\n\"\n","            f\"num_test_data: {len(test_data)}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"lora_r: {lora_r}\\n\"\n","            f\"lora_alpha: {lora_alpha}\\n\"\n","            f\"lora_dropout: {lora_dropout}\\n\"\n","            f\"lora_target_modules: {lora_target_modules}\\n\"\n","            f\"padding_side: {padding_side}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    class LlamaForMultilabelSequenceClassification(LlamaForSequenceClassification):\n","        def __init__(self, config):\n","            super().__init__(config)\n","\n","        def forward(self,\n","            input_ids=None,\n","            attention_mask=None,\n","            position_ids=None,\n","            past_key_values=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            use_cache = None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","            transformer_outputs = self.model(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                position_ids=position_ids,\n","                past_key_values=past_key_values,\n","                inputs_embeds=inputs_embeds,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            hidden_states = transformer_outputs[0]\n","            logits = self.score(hidden_states)\n","\n","            if input_ids is not None:\n","                batch_size = input_ids.shape[0]\n","            else:\n","                batch_size = inputs_embeds.shape[0]\n","\n","            if self.config.pad_token_id is None and batch_size != 1:\n","                raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n","            if self.config.pad_token_id is None:\n","                sequence_lengths = -1\n","            else:\n","                if input_ids is not None:\n","                    sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n","                else:\n","                    sequence_lengths = -1\n","\n","            pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n","\n","            loss = None\n","            if labels is not None:\n","                labels = labels.to(logits.device)\n","                loss_fct_pc = CrossEntropyLoss()\n","                loss_fct_cc = CrossEntropyLoss()\n","\n","                logits_pc = pooled_logits[:, :num_labels_pc]\n","                labels_onehot_pc = labels[:, :num_labels_pc]\n","                labels_pc = torch.argmax(labels_onehot_pc, axis=1)\n","\n","                logits_cc = pooled_logits[:, num_labels_pc:]\n","                labels_onehot_cc = labels[:, num_labels_pc:]\n","                labels_cc = torch.argmax(labels_onehot_cc, axis=1)\n","\n","                loss_pc = loss_fct_pc(logits_pc, labels_pc)\n","                loss_cc = loss_fct_cc(logits_cc, labels_cc)\n","                loss = loss_pc + 0.5*loss_cc\n","            if not return_dict:\n","                output = (pooled_logits,) + transformer_outputs[1:]\n","                return ((loss,) + output) if loss is not None else output\n","\n","            return SequenceClassifierOutputWithPast(\n","                loss=loss,\n","                logits=pooled_logits,\n","                past_key_values=transformer_outputs.past_key_values,\n","                hidden_states=transformer_outputs.hidden_states,\n","                attentions=transformer_outputs.attentions,\n","            )\n","\n","\n","    model = LlamaForMultilabelSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        load_in_8bit=True,\n","        problem_type=\"multi_label_classification\",\n","        torch_dtype=torch.float16,\n","        device_map=device_map,\n","        cache_dir=cache_dir)\n","\n","    tokenizer = LlamaTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","    # This is to fix the bad token in \"decapoda-research/llama-7b-hf\"\n","    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n","    model.config.bos_token_id = 1\n","    model.config.eos_token_id = 2\n","\n","    model = prepare_model_for_int8_training(model)\n","\n","    config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        target_modules=lora_target_modules,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=TaskType.SEQ_CLS,\n","        modules_to_save=None\n","    )\n","    model = get_peft_model(model, config)\n","\n","    if resume_from_checkpoint:\n","        # Check the available weights and load them\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"pytorch_model.bin\"\n","        )  # Full checkpoint\n","        if not os.path.exists(checkpoint_name):\n","            checkpoint_name = os.path.join(\n","                resume_from_checkpoint, \"adapter_model.bin\"\n","            )  # only LoRA model - LoRA config above has to fit\n","            resume_from_checkpoint = (\n","                False  # So the trainer won't try loading its state\n","            )\n","        # The two files above have a different name depending on how they were saved, but are actually the same.\n","        if os.path.exists(checkpoint_name):\n","            print(f\"Restarting from {checkpoint_name}\")\n","            adapters_weights = torch.load(checkpoint_name)\n","            set_peft_model_state_dict(model, adapters_weights)\n","        else:\n","            print(f\"Checkpoint {checkpoint_name} not found\")\n","\n","    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = Dataset.from_pandas(train_data)\n","    test_data = Dataset.from_pandas(test_data)\n","\n","    # train_data= train_data.shard(num_shards=5000, index=0)\n","    # test_data= test_data.shard(num_shards=5000, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","    tokenized_test = test_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","\n","    # default is padding to longest\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics_multi(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics_multi(y_pred=predictions, y=labels)\n","\n","    # Other hyperparameters to consider here is gradient_accumulation_steps, weight decay, learning rate, adam etype\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        fp16=True,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        remove_unused_columns=False,\n","        label_names=[\"labels\"],\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics_multi,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"ef33fa42","metadata":{"id":"ef33fa42"},"outputs":[],"source":["# Adopted from https://github.com/JHLiu7/EarlyDRGPrediction\n","\n","\n","import math\n","import pandas as pd\n","import numpy as np\n","import pickle as pk\n","import json\n","\n","\n","from sklearn.metrics import auc, roc_curve, precision_recall_curve, f1_score, accuracy_score\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from scipy.stats import pearsonr, spearmanr\n","\n","with open('paths.json', 'r') as f:\n","    path = json.load(f)\n","    drg_34_dissection_path = path[\"drg_34_dissection_path\"]\n","\n","# read a csv file but only read in the column of principal_diagnosis_lable and CC/MCC\n","pc_cc_mapping = pd.read_csv(drg_34_dissection_path)[[\"principal_diagnosis_lable\", \"CC/MCC\"]]\n","num_labels_pc = pc_cc_mapping.principal_diagnosis_lable.nunique()\n","\n","# make a dictinoary where key is principal_diagnosis_lable and value is CC/MCC. For one key there can be multiple values\n","pc_cc_dict = {}\n","for index, row in pc_cc_mapping.iterrows():\n","    if row[\"principal_diagnosis_lable\"] not in pc_cc_dict:\n","        pc_cc_dict[row[\"principal_diagnosis_lable\"]] = [row[\"CC/MCC\"]]\n","    else:\n","        pc_cc_dict[row[\"principal_diagnosis_lable\"]].append(row[\"CC/MCC\"])\n","\n","\n","\n","def map_rule(pred_pc, label_pc, pred_cc, label_cc):\n","    if pred_pc == label_pc:\n","        if pred_cc == label_cc:\n","            return True\n","        # if there's only one cc/MCC code of this principal diagnosis code, then any predcitons would be right\n","        elif len(pc_cc_dict[label_pc]) == 1:\n","            return True\n","        elif pred_cc in pc_cc_dict[label_pc] and pred_cc != label_cc:\n","            return False\n","        # for this group, default is to group 0\n","        elif set(pc_cc_dict[label_pc]) == {2, 1, 0}:\n","            mark_cc = 0\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","        # for this group, default is to group 3: without MCC\n","        elif set(pc_cc_dict[label_pc]) == {2, 3}:\n","            mark_cc = 3\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","        # for this group, there are two scenario. With MCC wil default to group 1, others will default to group 0\n","        elif set(pc_cc_dict[label_pc]) == {1, 0}:\n","            if pred_cc == 2:\n","                mark_cc = 1\n","            else:\n","                mark_cc = 0\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","    return False\n","\n","map_rule(12,12,1,2)\n","\n","\n","def accuracies_map(y_pred_pc, labels_pc, y_pred_cc, labels_cc):\n","    acc = 0.0\n","    num = len(y_pred_pc)\n","\n","    for i in range(num):\n","        if map_rule(y_pred_pc[i], labels_pc[i], y_pred_cc[i], labels_cc[i]):\n","            acc += 1.\n","    acc /= num\n","\n","    return acc\n","\n","\n","# full evaluation\n","def full_metrics(y_pred, y, drg_rule, d2i):\n","    y_pred_w, y_w = map2weight(y_pred, y, drg_rule=drg_rule, d2i=d2i)\n","\n","    reg_dict = reg_metrics(y_pred_w, y_w)\n","\n","    full_dict = {}\n","    full_dict.update(reg_dict)\n","\n","    cls_dict = cls_metrics(y_pred, y, len(d2i))\n","    full_dict.update(cls_dict)\n","\n","    return full_dict\n","\n","def cls_metrics(y_pred, y, class_num):\n","    # class_num = args.Y\n","    y_pred_ = softmax(y_pred)\n","    y_ = onehot_encode(y, class_num)\n","\n","    macroAUC, microAUC, appeared, cases = ave_auc_scores(y_pred_, y_)\n","    macroF1, microF1 = ave_f1_scores(y_pred, y)\n","\n","    metric_dict = {\n","        'microF1':microF1, 'macroF1':macroF1,\n","        'microAUC':microAUC, 'macroAUC':macroAUC,\n","        'labels': appeared, 'count': cases\n","    }\n","\n","    metric_dict['acc10'], metric_dict['acc5'], metric_dict['acc'], _ = accuracies(y_pred, y)\n","    return metric_dict\n","\n","def cls_metrics_eval(y_pred, y, class_num):\n","    # class_num = args.Y\n","    y_pred_ = softmax(y_pred)\n","    y_ = onehot_encode(y, class_num)\n","\n","    macroAUC, microAUC, appeared, cases = ave_auc_scores(y_pred_, y_)\n","    macroF1, microF1 = ave_f1_scores(y_pred, y)\n","\n","    metric_dict = {\n","        'microF1':microF1, 'macroF1':macroF1,\n","        'microAUC':microAUC, 'macroAUC':macroAUC,\n","        'labels': appeared, 'count': cases\n","    }\n","\n","    metric_dict['acc10'], metric_dict['acc5'], metric_dict['acc'], _ = accuracies(y_pred, y)\n","    metric_dict['y_label'] = y\n","    metric_dict['y_raw'] = y_pred\n","    # https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n","    metric_dict['y_raw_top5'] = (-y_pred).argsort(axis=1)[:, :5]\n","    metric_dict['y_pred'] = np.argmax(y_pred_, axis=1)\n","\n","    return metric_dict\n","\n","def cls_metrics_multi(y_pred, y):\n","    # class_num = args.Y\n","    predictions_pc = y_pred[:, :num_labels_pc]\n","    y_pred_pc = softmax(predictions_pc)\n","    labels_onehot_pc = y[:, :num_labels_pc]\n","    labels_pc = np.argmax(labels_onehot_pc, axis=1)\n","    predictions_cc = y_pred[:, num_labels_pc:]\n","    y_pred_cc = softmax(predictions_cc)\n","    labels_onehot_cc = y[:, num_labels_pc:]\n","    labels_cc = np.argmax(labels_onehot_cc, axis=1)\n","\n","    ## Need to double check it mirrows the original methods\n","\n","    macroAUC_pc, microAUC_pc, appeared_pc, cases_pc = ave_auc_scores(y_pred_pc, labels_onehot_pc)\n","    macroF1_pc, microF1_pc = ave_f1_scores(predictions_pc, labels_pc)\n","    acc10_pc, acc5_pc, acc_pc, _ = accuracies(predictions_pc, labels_pc)\n","\n","    macroAUC_cc, microAUC_cc, appeared_cc, cases_cc = ave_auc_scores(y_pred_cc, labels_onehot_cc)\n","    macroF1_cc, microF1_cc = ave_f1_scores(predictions_cc, labels_cc)\n","    acc10_cc, acc5_cc, acc_cc, _ = accuracies(predictions_cc, labels_cc)\n","\n","    y_pred_pc_single = np.argmax(y_pred_pc, axis=1)\n","    y_pred_cc_single = np.argmax(y_pred_cc, axis=1)\n","\n","    acc_map = accuracies_map(y_pred_pc_single, labels_pc, y_pred_cc_single, labels_cc)\n","\n","    metric_dict = {\n","        'acc_map': acc_map,\n","        'microF1_pc':microF1_pc, 'macroF1_pc':macroF1_pc,\n","        'microAUC_pc':microAUC_pc, 'macroAUC_pc':macroAUC_pc,\n","        'labels_pc': appeared_pc, 'count_pc': cases_pc,\n","        'acc10_pc': acc10_pc, 'acc5_pc': acc5_pc, 'acc_pc': acc_pc,\n","        'microF1_cc':microF1_cc, 'macroF1_cc':macroF1_cc,\n","        'microAUC_cc':microAUC_cc, 'macroAUC_cc':macroAUC_cc,\n","        'labels_cc': appeared_cc, 'count_cc': cases_cc,\n","        'acc10_cc': acc10_cc, 'acc5_cc': acc5_cc, 'acc_cc': acc_cc\n","    }\n","\n","    return metric_dict\n","\n","def reg_metrics(y_pred, y):\n","    mae = mean_absolute_error(y_pred, y)\n","    mse = mean_squared_error(y_pred,  y)\n","    spearman, p = spearmanr(y_pred, y)\n","\n","    metric_dict = {\n","        'MAE': mae, 'MSE': mse, 'RMSE': math.sqrt(mse),\n","        'spearman': spearman, 'corr_p': p\n","    }\n","\n","    dist= y_pred - y\n","    cmi = np.mean(dist)\n","    overshot, undershot = len(dist[dist>0]), len(dist[dist<0])\n","\n","    metric_dict.update({\n","        'CMI_error': cmi/np.mean(y), 'CMI_raw':cmi, 'overshot': overshot, 'undershot': undershot\n","    })\n","    return metric_dict\n","\n","def cls_metrics_multi_eval(y_pred, y):\n","    # class_num = args.Y\n","\n","    predictions_pc = y_pred[:, :num_labels_pc]\n","    y_pred_pc = softmax(predictions_pc)\n","    labels_onehot_pc = y[:, :num_labels_pc]\n","    labels_pc = np.argmax(labels_onehot_pc, axis=1)\n","    predictions_cc = y_pred[:, num_labels_pc:]\n","    y_pred_cc = softmax(predictions_cc)\n","    labels_onehot_cc = y[:, num_labels_pc:]\n","    labels_cc = np.argmax(labels_onehot_cc, axis=1)\n","\n","    ## Need to double check it mirrows the original methods\n","\n","    macroAUC_pc, microAUC_pc, appeared_pc, cases_pc = ave_auc_scores(y_pred_pc, labels_onehot_pc)\n","    macroF1_pc, microF1_pc = ave_f1_scores(predictions_pc, labels_pc)\n","    acc10_pc, acc5_pc, acc_pc, _ = accuracies(predictions_pc, labels_pc)\n","\n","    macroAUC_cc, microAUC_cc, appeared_cc, cases_cc = ave_auc_scores(y_pred_cc, labels_onehot_cc)\n","    macroF1_cc, microF1_cc = ave_f1_scores(predictions_cc, labels_cc)\n","    acc10_cc, acc5_cc, acc_cc, _ = accuracies(predictions_cc, labels_cc)\n","\n","    y_pred_pc_single = np.argmax(y_pred_pc, axis=1)\n","    y_pred_cc_single = np.argmax(y_pred_cc, axis=1)\n","\n","    acc_map = accuracies_map(y_pred_pc_single, labels_pc, y_pred_cc_single, labels_cc)\n","\n","    metric_dict = {\n","        'acc_map': acc_map,\n","        'microF1_pc':microF1_pc, 'macroF1_pc':macroF1_pc,\n","        'microAUC_pc':microAUC_pc, 'macroAUC_pc':macroAUC_pc,\n","        'labels_pc': appeared_pc, 'count_pc': cases_pc,\n","        'acc10_pc': acc10_pc, 'acc5_pc': acc5_pc, 'acc_pc': acc_pc,\n","        'microF1_cc':microF1_cc, 'macroF1_cc':macroF1_cc,\n","        'microAUC_cc':microAUC_cc, 'macroAUC_cc':macroAUC_cc,\n","        'labels_cc': appeared_cc, 'count_cc': cases_cc,\n","        'acc10_cc': acc10_cc, 'acc5_cc': acc5_cc, 'acc_cc': acc_cc\n","    }\n","\n","    metric_dict['y_label'] = y\n","    metric_dict['y_raw'] = y_pred\n","\n","    return metric_dict\n","\n","\n","# to print out results\n","def result2str(d):\n","    try:\n","        mif, maf = d['microF1'], d['macroF1']\n","        mia, maa = d['microAUC'], d['macroAUC']\n","        a10, a5, a = d['acc10'], d['acc5'], d['acc']\n","        la, ct = d['labels'], d['count']\n","    except:\n","        pass\n","    ma, rm = d['MAE'], d['RMSE']\n","    sp, p = d['spearman'], d['corr_p']\n","    cm,ov,ud = d['CMI_error'], d['overshot'], d['undershot']\n","\n","    title = \"****\" * 5 + '\\n'\n","    try:\n","        s1 = \"{} cases, {} labels\".format(ct, la)\n","        s2 = \"MACRO-AUC     \\tMICRO-AUC      \\tMACRO-F1     \\tMICRO-F1  \"\n","        s3 = \"{:.4f}  \\t{:.4f}  \\t{:.4f}  \\t{:.4f}\".format(maa, mia, maf, mif)\n","        s4 = \"Acc10  \\tAcc5  \\tAcc \"\n","        s5 = \"{:.4f}  \\t{:.4f}  \\t{:.4f}  \\n\".format(a10, a5, a)\n","        title = title+'\\n'.join([s1, s2, s3, s4, s5])\n","    except:\n","        pass\n","    r1 = \"MAE: {:.4f}  RMSE: {:.4f}  Corr: {:.4f}  \\n\".format(ma, rm, sp)\n","    r2 = \"CMI_error: {:.2%}  overshot: {}  undershot: {}  \\n\\n\".format(cm,ov,ud)\n","\n","    title = title+'\\n'.join([r1, r2])\n","\n","    return title\n","\n","\n","# running evaluation\n","def score_f1(y_pred, y):\n","    \"\"\"\n","        y_pred: logit\n","    \"\"\"\n","    y_flat = np.argmax(y_pred, axis=1)\n","    return f1_score(y, y_flat, average='micro')\n","\n","def score_mae(y_pred, y):\n","    return mean_absolute_error(y_pred, y)\n","\n","\n","# utils\n","def map2weight(y_pred, y, drg_rule, d2i):\n","\n","    idx2drg = {v:k for k,v in d2i.items()}\n","    drg2weight = {}\n","    for _, row in drg_rule.iterrows():\n","        drg2weight[row['DRG_CODE']] = row['WEIGHT']\n","\n","    y_pred = [drg2weight[idx2drg[d]] for d in np.argmax(y_pred, axis=1)]\n","    y = [drg2weight[idx2drg[d]] for d in y]\n","    return np.array(y_pred), np.array(y)\n","\n","def softmax(x):\n","    e_x = np.exp(x)\n","    return e_x / np.expand_dims(e_x.sum(axis=1), 1)\n","\n","def onehot_encode(y, class_num):\n","    \"\"\"\n","        y: a flat array of labels\n","    \"\"\"\n","    yone = []\n","    for i in y:\n","        onehot = np.zeros(class_num)\n","        onehot[i] = 1\n","        yone.append(onehot)\n","    return np.array(yone)\n","\n","def accuracies(y_pred, y, onlyAcc=False):\n","    \"\"\"\n","    y_pred: logits\n","    y: a list of labels\n","    \"\"\"\n","    acc10 = 0.0\n","    acc5 = 0.0\n","    acc1 = 0.0\n","    num = len(y)\n","\n","    for i in range(num):\n","\n","        pred = y_pred[i]\n","        top10_pred = set(pred.argsort()[-10:])\n","        top5_pred = set(pred.argsort()[-5:])\n","        top1_pred = set(pred.argsort()[-1:])\n","\n","        label = y[i]\n","        # label = np.argmax(y[i])\n","\n","        if label in top10_pred:\n","            acc10 += 1.\n","        if label in top5_pred:\n","            acc5 += 1.\n","        if label in top1_pred:\n","            acc1 += 1.\n","\n","    acc10 /= num\n","    acc5 /= num\n","    acc1 /= num\n","\n","    if onlyAcc:\n","        return acc1\n","    return acc10, acc5, acc1, num\n","\n","def ave_auc_scores(y_pred, y):\n","    # micro/macro auc based on classes\n","    \"\"\"\n","        y.shape: [sample, classes] float\n","        y_pred.shape: [sample, classes] int\n","        numpy\n","    \"\"\"\n","\n","    aucroc_cases = {}\n","    for i in range(y.shape[1]):\n","        if y[:, i].sum()>0: # class appears in test set\n","            fp, tp, _ = roc_curve(y[:, i], y_pred[:, i])\n","            if len(fp) >1 and len(tp) >1:\n","                auc_roc = auc(fp, tp)\n","                aucroc_cases[i] = auc_roc\n","\n","    fp_mic, tp_mic, _ = roc_curve(y.ravel(), y_pred.ravel())\n","\n","    # appearing classes\n","    labels = list(aucroc_cases.keys())\n","\n","    # roc\n","    auc_roc_macro = np.mean(list(aucroc_cases.values()))\n","    auc_roc_micro = auc(fp_mic, tp_mic)\n","    return auc_roc_macro, auc_roc_micro, len(labels), len(y)\n","\n","def ave_f1_scores(y_pred, y):\n","    # f1\n","    # require y_pred, y being flat list\n","    y_flat = np.argmax(y_pred, axis=1)\n","\n","    f1_macro = f1_score(y, y_flat, average='macro', labels=np.unique(y))\n","    f1_micro = f1_score(y, y_flat, average='micro', labels=np.unique(y))\n","\n","    return f1_macro, f1_micro\n"]},{"cell_type":"code","execution_count":null,"id":"1f405343","metadata":{"id":"1f405343"},"outputs":[],"source":["import os\n","\n","def create_folder(parent_path, folder):\n","    if not parent_path.endswith('/'):\n","        parent_path += '/'\n","    folder_path = parent_path + folder\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","    return folder_path"]}],"metadata":{"colab":{"provenance":[{"file_id":"1aKrI9p_ubdupi_95PZLK3XmFoNv3_lvU","timestamp":1718091733781}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}