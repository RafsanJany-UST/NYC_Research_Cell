{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61304,"status":"ok","timestamp":1727345004012,"user":{"displayName":"Rafsan Jany","userId":"08721334090885411304"},"user_tz":-360},"id":"0r6E_q4WsSI5","outputId":"7cd246a6-a7b7-4d43-cdb0-61d98b63e87f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.1/465.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","#df = pd.read_excel(\"/content/drive/MyDrive/Tanjila_mam/DS-Healthcare_version_2.xlsx\")\n","\n","!pip install -q flwr[\"simulation\"] tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1nWfKnoKdldvxYnXfokVwnHF01BKmmbY0"},"id":"XyHHZglz6uIu","outputId":"0324d57f-bb06-4969-98d3-2349c15c3513"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import math\n","from typing import Dict, List, Tuple\n","from imblearn.over_sampling import SMOTE\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    f1_score,\n","    recall_score,\n","    precision_score,\n","    roc_auc_score,\n",")\n","import flwr as fl\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from flwr.common import Metrics\n","from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n","\n","\n","import math\n","from typing import Dict, List, Tuple\n","from imblearn.over_sampling import SMOTE\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from flwr.common import Metrics\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    f1_score,\n","    recall_score,\n","    precision_score,\n","    roc_auc_score,\n",")\n","import flwr as fl\n","import numpy as np\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","\n","num_of_clients = [10,20,30,40,50]\n","\n","\n","for NUM_CLIENTS in tqdm(num_of_clients):\n","\n","  df = pd.read_excel(\"/content/drive/MyDrive/Tanjila_mam/DS-Healthcare_version_2.xlsx\")\n","\n","  VERBOSE = 0\n","  #NUM_CLIENTS = 30\n","  count_rounds = 0\n","  number_of_rounds = 15\n","  saving_model_name = \"\"\n","  metrics_dict = {\"client\": [], \"centralized\": []}  # Dictionary to store metrics\n","\n","\n","  # ANN Model\n","  def get_model():\n","      model = tf.keras.models.Sequential(\n","          [\n","              tf.keras.layers.Flatten(input_shape=[9]),\n","              tf.keras.layers.Dense(128, activation=\"relu\"),\n","              tf.keras.layers.Dropout(0.2),\n","              tf.keras.layers.Dense(3, activation=\"softmax\"),\n","          ]\n","      )\n","      global saving_model_name, NUM_CLIENTS, number_of_rounds\n","      saving_model_name = str(NUM_CLIENTS) + \"_\" + str(number_of_rounds) + \"_ann\"\n","      model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","      return model\n","\n","\n","  # Flower Client\n","  class FlowerClient(fl.client.NumPyClient):\n","      def __init__(self, x_train, y_train, x_val, y_val) -> None:\n","          self.model = get_model()\n","          self.x_train, self.y_train = x_train, y_train\n","          self.x_val, self.y_val = x_val, y_val\n","\n","      def get_parameters(self, config):\n","          return self.model.get_weights()\n","\n","      def fit(self, parameters, config):\n","          self.model.set_weights(parameters)\n","          history = self.model.fit(\n","              self.x_train, self.y_train, epochs=1, batch_size=32, verbose=VERBOSE\n","          )\n","          train_loss = history.history[\"loss\"][-1]\n","          train_acc = history.history[\"accuracy\"][-1]\n","\n","          return self.model.get_weights(), len(self.x_train), {\n","              \"train_loss\": train_loss,\n","              \"train_accuracy\": train_acc,\n","          }\n","\n","      def evaluate(self, parameters, config):\n","          self.model.set_weights(parameters)\n","          y_pred = np.argmax(self.model.predict(self.x_val, verbose=VERBOSE), axis=1)\n","          loss, acc = self.model.evaluate(self.x_val, self.y_val, batch_size=64, verbose=VERBOSE)\n","\n","          # Confusion Matrix and Metrics\n","          cm = confusion_matrix(self.y_val, y_pred)\n","          f1 = f1_score(self.y_val, y_pred, average=\"weighted\")\n","          precision = precision_score(self.y_val, y_pred, average=\"weighted\")\n","          recall = recall_score(self.y_val, y_pred, average=\"weighted\")\n","          auc = roc_auc_score(self.y_val, self.model.predict(self.x_val), multi_class=\"ovr\")\n","\n","          misclassification_rate = 1 - acc\n","          fpr = cm[0][1] / (cm[0][1] + cm[0][0])\n","          fnr = cm[1][0] / (cm[1][0] + cm[1][1])\n","\n","          # Flatten confusion matrix to avoid passing list of lists\n","          flat_confusion_matrix = np.array(cm).flatten().tolist()\n","\n","          metrics = {\n","              \"test_accuracy\": acc,\n","              \"test_loss\": loss,\n","              \"f1_score\": f1,\n","              \"precision\": precision,\n","              \"recall\": recall,\n","              \"auc\": auc,\n","              \"misclassification_rate\": misclassification_rate,\n","              \"fpr\": fpr,\n","              \"fnr\": fnr,\n","              \"confusion_matrix\": flat_confusion_matrix,\n","          }\n","\n","          metrics_dict[\"client\"].append(metrics)  # Store client metrics\n","\n","          return loss, len(self.x_val), metrics\n","\n","\n","  # Partition Dataset\n","  def partition_dataset():\n","      encoder = LabelEncoder()\n","      df['Type'] = encoder.fit_transform(df['Type'])\n","      X = df.iloc[:, 4:]\n","      y = df['Type']\n","\n","      # Apply SMOTE to balance the dataset\n","      smote = SMOTE(random_state=42)\n","      X_resampled, y_resampled = smote.fit_resample(X, y)\n","\n","      x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=0)\n","\n","      x_train = x_train.to_numpy()\n","      x_test = x_test.to_numpy()\n","      y_train = y_train.to_numpy()\n","      y_test = y_test.to_numpy()\n","\n","      testset = (x_test, y_test)\n","      partitions = []\n","      partition_size = math.floor(len(x_train) / NUM_CLIENTS)\n","      for cid in range(NUM_CLIENTS):\n","          idx_from, idx_to = int(cid) * partition_size, (int(cid) + 1) * partition_size\n","          partitions.append((x_train[idx_from:idx_to], y_train[idx_from:idx_to]))\n","      return partitions, testset\n","\n","\n","  # Client function\n","  def get_client_fn(dataset_partitions):\n","      def client_fn(cid: str) -> fl.client.Client:\n","          x_train, y_train = dataset_partitions[int(cid)]\n","          split_idx = math.floor(len(x_train) * 0.9)\n","          x_train_cid, y_train_cid = x_train[:split_idx], y_train[:split_idx]\n","          x_val_cid, y_val_cid = x_train[split_idx:], y_train[split_idx:]\n","          return FlowerClient(x_train_cid, y_train_cid, x_val_cid, y_val_cid)\n","      return client_fn\n","\n","\n","  # Centralized (Global) evaluation function\n","  def get_evaluate_fn(testset):\n","      x_test, y_test = testset\n","\n","      def evaluate(server_round: int, parameters: fl.common.NDArrays, config: Dict[str, fl.common.Scalar]):\n","          model = get_model()\n","          model.set_weights(parameters)\n","          y_pred = np.argmax(model.predict(x_test, verbose=VERBOSE), axis=1)\n","          loss, accuracy = model.evaluate(x_test, y_test, verbose=VERBOSE)\n","\n","          # Confusion Matrix and Metrics\n","          cm = confusion_matrix(y_test, y_pred)\n","          f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","          precision = precision_score(y_test, y_pred, average=\"weighted\")\n","          recall = recall_score(y_test, y_pred, average=\"weighted\")\n","          auc = roc_auc_score(y_test, model.predict(x_test), multi_class=\"ovr\")\n","\n","          misclassification_rate = 1 - accuracy\n","          fpr = cm[0][1] / (cm[0][1] + cm[0][0])\n","          fnr = cm[1][0] / (cm[1][0] + cm[1][1])\n","\n","          # Flatten confusion matrix\n","          flat_confusion_matrix = np.array(cm).flatten().tolist()\n","\n","          global count_rounds\n","          global number_of_rounds\n","\n","          if number_of_rounds - count_rounds == 0:\n","              global saving_model_name\n","              model.save(saving_model_name + \"_model.h5\")\n","\n","          count_rounds += 1\n","\n","          metrics = {\n","              \"test_accuracy\": accuracy,\n","              \"test_loss\": loss,\n","              \"f1_score\": f1,\n","              \"precision\": precision,\n","              \"recall\": recall,\n","              \"auc\": auc,\n","              \"misclassification_rate\": misclassification_rate,\n","              \"fpr\": fpr,\n","              \"fnr\": fnr,\n","              \"confusion_matrix\": flat_confusion_matrix,\n","          }\n","\n","          metrics_dict[\"centralized\"].append(metrics)  # Store centralized metrics\n","\n","          return loss, {\n","              \"accuracy\": accuracy,\n","              \"loss\": loss,\n","              \"f1_score\": f1,\n","              \"precision\": precision,\n","              \"recall\": recall,\n","              \"auc\": auc,\n","              \"misclassification_rate\": misclassification_rate,\n","              \"fpr\": fpr,\n","              \"fnr\": fnr,\n","              \"confusion_matrix\": flat_confusion_matrix,\n","          }\n","\n","      return evaluate\n","\n","\n","  # Aggregation function for metrics (weighted average)\n","  def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n","      accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n","      losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n","      f1_scores = [num_examples * m[\"f1_score\"] for num_examples, m in metrics]\n","      precisions = [num_examples * m[\"precision\"] for num_examples, m in metrics]\n","      recalls = [num_examples * m[\"recall\"] for num_examples, m in metrics]\n","      aucs = [num_examples * m[\"auc\"] for num_examples, m in metrics]\n","      misclassification_rates = [num_examples * m[\"misclassification_rate\"] for num_examples, m in metrics]\n","      fprs = [num_examples * m[\"fpr\"] for num_examples, m in metrics]\n","      fnrs = [num_examples * m[\"fnr\"] for num_examples, m in metrics]\n","      cm = [m[\"confusion_matrix\"] for _, m in metrics]\n","\n","      total_examples = sum([num_examples for num_examples, _ in metrics])\n","\n","      return {\n","          \"accuracy\": sum(accuracies) / total_examples,\n","          \"loss\": sum(losses) / total_examples,\n","          \"f1_score\": sum(f1_scores) / total_examples,\n","          \"precision\": sum(precisions) / total_examples,\n","          \"recall\": sum(recalls) / total_examples,\n","          \"auc\": sum(aucs) / total_examples,\n","          \"misclassification_rate\": sum(misclassification_rates) / total_examples,\n","          \"fpr\": sum(fprs) / total_examples,\n","          \"fnr\": sum(fnrs) / total_examples,\n","          \"confusion_matrix\": np.mean(cm, axis=0).flatten().tolist(),\n","      }\n","\n","\n","\n","  # Partition data and start Flower simulation\n","  dataset_partitions, testset = partition_dataset()\n","  client_fn = get_client_fn(dataset_partitions)\n","  evaluate_fn = get_evaluate_fn(testset)\n","\n","  # Start FL Simulation\n","  fl.simulation.start_simulation(\n","      client_fn=client_fn,\n","      num_clients=NUM_CLIENTS,\n","      config=fl.server.ServerConfig(num_rounds=number_of_rounds),\n","      strategy=fl.server.strategy.FedAvg(\n","          evaluate_fn=evaluate_fn,\n","          fraction_fit=0.1,\n","          fraction_evaluate=0.1,\n","          min_fit_clients=NUM_CLIENTS,\n","          min_evaluate_clients=NUM_CLIENTS,\n","          min_available_clients=NUM_CLIENTS,\n","          on_fit_config_fn=lambda rnd: {\"rnd\": rnd},\n","          on_evaluate_config_fn=lambda rnd: {\"rnd\": rnd},\n","          evaluate_metrics_aggregation_fn=weighted_average,\n","      ),\n","  )\n","\n","\n","\n","\n","\n","\n","  centralized_metrics = {\n","      'test_accuracy': [],\n","      'test_loss': [],\n","      'f1_score': [],\n","      'precision': [],\n","      'recall': [],\n","      'auc': [],\n","      'misclassification_rate': [],\n","      'fpr': [],\n","      'fnr': [],\n","      'confusion_matrix': []\n","  }\n","\n","  # Extract the values for each metric from each centralized result\n","  for entry in metrics_dict['centralized']:\n","      for key in centralized_metrics.keys():\n","          centralized_metrics[key].append(entry[key])\n","\n","  # Print the reformatted dictionary\n","  print(centralized_metrics)\n","\n","\n","\n","\n","\n","  import json\n","\n","  # Path to your JSON file\n","  json_file_path = '/content/drive/MyDrive/Tanjila_mam/JSON Result/ANN_json_file_number_of_client.json'\n","\n","  # Load the existing JSON file\n","  try:\n","      with open(json_file_path, 'r') as file:\n","          data = json.load(file)\n","  except FileNotFoundError:\n","      # If the file doesn't exist, initialize with an empty structure\n","      data = {}\n","\n","  # Ensure 'centralized_metrics' is a list\n","  if 'centralized_metrics' not in data:\n","      data['centralized_metrics'] = []\n","\n","  # Append the new centralized metrics\n","  if isinstance(data['centralized_metrics'], list):\n","      data['centralized_metrics'].append(centralized_metrics)\n","  else:\n","      # If 'centralized_metrics' is not a list, convert it to a list and append the new data\n","      data['centralized_metrics'] = [data['centralized_metrics'], centralized_metrics]\n","\n","  # Write the updated data back to the JSON file\n","  with open(json_file_path, 'w') as file:\n","      json.dump(data, file, indent=4)\n","  print(\"___________________________________________________________________________________________________________________________________\")\n","  print(\"___________________________________________________________________________________________________________________________________\")\n","  print(\"New centralized metrics appended and JSON file updated successfully for Clients:\",NUM_CLIENTS,\" Number of Rouns: \",number_of_rounds)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrOs1-Mu-oCu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVD8lqsC-oA2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZ4K3v-v-n-X"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cn-XWMEb-n8Z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9vWnec6-n6E"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":563,"status":"ok","timestamp":1727246036619,"user":{"displayName":"Rafsan Jany","userId":"08721334090885411304"},"user_tz":-360},"id":"7h8NHAtSlI5w","outputId":"4db98f56-8790-4814-e3aa-5d8346fc2baa"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'test_accuracy': [0.3337828814983368, 0.8500904440879822, 0.8740978240966797, 0.8773615956306458, 0.8871893882751465, 0.8986856341362], 'test_loss': [11.377601623535156, 0.4172004461288452, 0.3452754020690918, 0.3227986991405487, 0.31076815724372864, 0.29018548130989075], 'f1_score': [0.16706018188578928, 0.8485856087604265, 0.8730831705416685, 0.8759081218145705, 0.8860897207083667, 0.8978690808875907], 'precision': [0.11141100444687232, 0.8600342045122951, 0.8798852430192183, 0.8858205984066837, 0.8932858690436479, 0.9029220979020873], 'recall': [0.3337828702118674, 0.8500904577108029, 0.8740978125048878, 0.8773615892750697, 0.8871894154919909, 0.8986856260003788], 'auc': [0.5277273309664025, 0.9556364653292508, 0.9708721822200772, 0.9758055055394955, 0.9772346008519502, 0.9800351615694165], 'misclassification_rate': [0.6662171185016632, 0.14990955591201782, 0.1259021759033203, 0.12263840436935425, 0.11281061172485352, 0.10131436586380005], 'fpr': [0.0, 0.057055947388996886, 0.0921671981085009, 0.08564667674442207, 0.07067395655043272, 0.052070518512179205], 'fnr': [1.0, 0.07639242385679137, 0.028530206447889613, 0.020017588676242724, 0.024057826781552, 0.028134820124226214], 'confusion_matrix': [[192061, 0, 0, 191931, 0, 0, 191415, 0, 0], [176075, 10654, 5332, 14407, 174185, 3339, 31166, 21361, 138888], [167408, 16996, 7657, 5438, 185167, 1326, 22178, 18850, 150387], [171177, 16034, 4850, 3824, 187208, 899, 23551, 21409, 146455], [173205, 13172, 5684, 4593, 186322, 1016, 20492, 19955, 150968], [177223, 9735, 5103, 5354, 184944, 1633, 23253, 13219, 154943]]}\n"]}],"source":["centralized_metrics = {\n","    'test_accuracy': [],\n","    'test_loss': [],\n","    'f1_score': [],\n","    'precision': [],\n","    'recall': [],\n","    'auc': [],\n","    'misclassification_rate': [],\n","    'fpr': [],\n","    'fnr': [],\n","    'confusion_matrix': []\n","}\n","\n","# Extract the values for each metric from each centralized result\n","for entry in metrics_dict['centralized']:\n","    for key in centralized_metrics.keys():\n","        centralized_metrics[key].append(entry[key])\n","\n","# Print the reformatted dictionary\n","print(centralized_metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":549,"status":"ok","timestamp":1727246774765,"user":{"displayName":"Rafsan Jany","userId":"08721334090885411304"},"user_tz":-360},"id":"JgEPoUyKyWUK","outputId":"18cf47b9-7bee-4f1d-f612-b0893ad22dc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["New centralized metrics appended and JSON file updated successfully!\n"]}],"source":["import json\n","\n","# Path to your JSON file\n","json_file_path = 'your_json_file.json'\n","\n","# New centralized_metrics data\n","new_centralized_metrics = {\n","    'test_accuracy': [0.3337828814983368,  0.8871893882751465, 0.8986856341362],\n","    'test_loss': [ 0.31076815724372864, 0.29018548130989075],\n","    'f1_score': [0.16706018188578928,0.8978690808875907],\n","    'precision': [0.11141100444687232,  0.9029220979020873],\n","    'recall': [0.3337828702118674, 0.8986856260003788],\n","    'auc': [0.5277273309664025,  0.9800351615694165],\n","    'misclassification_rate': [0.6662171185016632, 0.11281061172485352, 0.10131436586380005],\n","    'fpr': [0.0, 0.052070518512179205],\n","    'fnr': [1.0, 0.024057826781552, 0.028134820124226214],\n","    'confusion_matrix': [\n","        [192061, 191415, 0, 0],\n","        [176075, 10654, 5332, 14407, 174185, 3339, 31166, 21361, 138888],\n","        [167408, 16996, 7657, 5438, 185167, 1326, 22178, 18850, 150387],\n","        [171177, 16034, 4850, 3824, 187208, 899, 23551, 21409, 146455],\n","        [173205, 13172, 5684, 4593, 186322, 1016, 20492, 19955, 150968],\n","        [177223, 9735, 5103, 5354, 184944, 1633, 23253, 13219, 154943]\n","    ]\n","}\n","\n","# Load the existing JSON file\n","try:\n","    with open(json_file_path, 'r') as file:\n","        data = json.load(file)\n","except FileNotFoundError:\n","    # If the file doesn't exist, initialize with an empty structure\n","    data = {}\n","\n","# Ensure 'centralized_metrics' is a list\n","if 'centralized_metrics' not in data:\n","    data['centralized_metrics'] = []\n","\n","# Append the new centralized metrics\n","if isinstance(data['centralized_metrics'], list):\n","    data['centralized_metrics'].append(new_centralized_metrics)\n","else:\n","    # If 'centralized_metrics' is not a list, convert it to a list and append the new data\n","    data['centralized_metrics'] = [data['centralized_metrics'], new_centralized_metrics]\n","\n","# Write the updated data back to the JSON file\n","with open(json_file_path, 'w') as file:\n","    json.dump(data, file, indent=4)\n","\n","print(\"New centralized metrics appended and JSON file updated successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vN1KI307qB5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1mkZVJ47p-l"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCmsQ1JwxXOJ0Yh8pc+8BQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}