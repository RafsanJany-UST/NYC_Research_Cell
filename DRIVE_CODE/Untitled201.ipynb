{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZQbmFQd6r9xPq/gAk5Y04"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"i6yPRM3vFePr"},"outputs":[],"source":["import os\n","from typing import List\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","import fire\n","import torch\n","from datasets import load_dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import json\n","\n","from huggingface_hub import notebook_login\n","import numpy as np\n","\n","from utils.eval_utils import cls_metrics\n","from utils.gen_utils import create_folder\n","\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        train_set_path = path[\"train_set_path\"]\n","        test_set_path = path[\"test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","\n","def train(\n","    base_model: str = \"emilyalsentzer/Bio_ClinicalBERT\",  # the only required argument\n","    train_data_path: str = train_set_path,\n","    val_data_path: str = test_set_path,\n","    cache_dir: str = catche_path,\n","    split: int = 100,\n","    micro_batch_size: int = 16, # based on the previous recommended practice for classification-oriented fine-tuning of BERT (Devlin et al. 2018; Adhikari et al. 2019)\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512, # consider changing to 1024\n","    model_name: str = \"bert\",\n","    wandb_project: str = \"classification\", #other options: \"generative\", \"multilabel-classification\",\n","    wandb_watch: str = \"gradients\",  # options: false | gradients | all ; issues when using all: I have since bypassed this issue by only logging gradient and instead of all.\n","    wandb_log_model: str = \"\",  # options: false | true\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_name}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels = pd.read_csv(train_data_path).label.nunique()\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        cache_dir=cache_dir)\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","\n","    def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            all_param += param.numel()\n","            if param.requires_grad:\n","                trainable_params += param.numel()\n","        print(\n","            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","        )\n","\n","    print_trainable_parameters(model)\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = load_dataset(\"csv\", data_files=train_data_path, split=f'train[:{split}%]')\n","    test_data = load_dataset(\"csv\", data_files=val_data_path, split=f'train[:{split}%]')\n","\n","    train_data= train_data.shard(num_shards=5000, index=0)\n","    test_data= test_data.shard(num_shards=2000, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True)\n","    tokenized_test = test_data.map(preprocess_function, batched=True)\n","\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics(predictions, labels, class_num=num_labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]}]}