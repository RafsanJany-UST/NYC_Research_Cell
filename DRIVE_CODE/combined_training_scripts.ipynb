{"cells":[{"cell_type":"code","source":[],"metadata":{"id":"-gd3ng-DKTGS"},"id":"-gd3ng-DKTGS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sEbKCkn4KS82"},"id":"sEbKCkn4KS82","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d9794339","metadata":{"id":"d9794339"},"source":["# Combined Training Scripts"]},{"cell_type":"code","execution_count":null,"id":"d41faf97","metadata":{"id":"d41faf97"},"outputs":[],"source":["import os\n","from typing import List\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","import fire\n","import torch\n","from datasets import load_dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import json\n","\n","from huggingface_hub import notebook_login\n","import numpy as np\n","\n","from utils.eval_utils import cls_metrics\n","from utils.gen_utils import create_folder\n","\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        train_set_path = path[\"train_set_path\"]\n","        test_set_path = path[\"test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","\n","def train(\n","    base_model: str = \"emilyalsentzer/Bio_ClinicalBERT\",  # the only required argument\n","    train_data_path: str = train_set_path,\n","    val_data_path: str = test_set_path,\n","    cache_dir: str = catche_path,\n","    split: int = 100,\n","    micro_batch_size: int = 16, # based on the previous recommended practice for classification-oriented fine-tuning of BERT (Devlin et al. 2018; Adhikari et al. 2019)\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512, # consider changing to 1024\n","    model_name: str = \"bert\",\n","    wandb_project: str = \"classification\", #other options: \"generative\", \"multilabel-classification\",\n","    wandb_watch: str = \"gradients\",  # options: false | gradients | all ; issues when using all: I have since bypassed this issue by only logging gradient and instead of all.\n","    wandb_log_model: str = \"\",  # options: false | true\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_name}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels = pd.read_csv(train_data_path).label.nunique()\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        cache_dir=cache_dir)\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","\n","    def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            all_param += param.numel()\n","            if param.requires_grad:\n","                trainable_params += param.numel()\n","        print(\n","            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","        )\n","\n","    print_trainable_parameters(model)\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = load_dataset(\"csv\", data_files=train_data_path, split=f'train[:{split}%]')\n","    test_data = load_dataset(\"csv\", data_files=val_data_path, split=f'train[:{split}%]')\n","\n","    train_data= train_data.shard(num_shards=5000, index=0)\n","    test_data= test_data.shard(num_shards=2000, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True)\n","    tokenized_test = test_data.map(preprocess_function, batched=True)\n","\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics(predictions, labels, class_num=num_labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"bbf01751","metadata":{"id":"bbf01751"},"outputs":[],"source":["# Adopted framework from: https://github.com/tloen/alpaca-lora\n","\n","\n","import os\n","import pandas as pd\n","from datetime import datetime\n","import json\n","from typing import List\n","\n","import fire\n","import torch\n","from datasets import load_dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import LlamaTokenizer, LlamaForSequenceClassification\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n","    TaskType\n",")\n","import torch\n","\n","from utils.eval_utils import cls_metrics\n","from utils.gen_utils import create_folder\n","\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        train_set_path = path[\"train_set_path\"]\n","        test_set_path = path[\"test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","\n","def train(\n","    # model/data params\n","    base_model: str = \"decapoda-research/llama-7b-hf\",  # the only required argument\n","    model_size: str = \"7b\",\n","    train_data_path: str = train_set_path,\n","    val_data_path: str = test_set_path,\n","    split: int = 100,\n","    cache_dir: str = catche_path,\n","    micro_batch_size: int = 4,\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512,\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"score\"\n","    ],\n","    padding_side: str = \"right\",\n","    wandb_project: str = \"classification\",\n","    wandb_watch: str = \"gradients\",\n","    wandb_log_model: str = \"\",\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_size}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{padding_side}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels = pd.read_csv(train_data_path).label.nunique()\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"model_size: {model_size}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"lora_r: {lora_r}\\n\"\n","            f\"lora_alpha: {lora_alpha}\\n\"\n","            f\"lora_dropout: {lora_dropout}\\n\"\n","            f\"lora_target_modules: {lora_target_modules}\\n\"\n","            f\"padding_side: {padding_side}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    model = LlamaForSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        load_in_8bit=True,\n","        torch_dtype=torch.float16,\n","        device_map=device_map,\n","        cache_dir=cache_dir)\n","\n","    tokenizer = LlamaTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","    # This is to fix the bad token in \"decapoda-research/llama-7b-hf\"\n","\n","    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n","    model.config.bos_token_id = 1\n","    model.config.eos_token_id = 2\n","\n","    model = prepare_model_for_int8_training(model)\n","\n","    # note when passing task type as string argument, it will lead to error. May consider adding module_to_save manually\n","    config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        target_modules=lora_target_modules,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=TaskType.SEQ_CLS,\n","        modules_to_save=None,\n","    )\n","    model = get_peft_model(model, config)\n","\n","    if resume_from_checkpoint:\n","        # Check the available weights and load them\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"pytorch_model.bin\"\n","        )  # Full checkpoint\n","        if not os.path.exists(checkpoint_name):\n","            checkpoint_name = os.path.join(\n","                resume_from_checkpoint, \"adapter_model.bin\"\n","            )  # only LoRA model - LoRA config above has to fit\n","            resume_from_checkpoint = (\n","                False  # So the trainer won't try loading its state\n","            )\n","        # The two files above have a different name depending on how they were saved, but are actually the same.\n","        if os.path.exists(checkpoint_name):\n","            print(f\"Restarting from {checkpoint_name}\")\n","            adapters_weights = torch.load(checkpoint_name)\n","            set_peft_model_state_dict(model, adapters_weights)\n","        else:\n","            print(f\"Checkpoint {checkpoint_name} not found\")\n","\n","    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = load_dataset(\"csv\", data_files=train_data_path, split=f'train[:{split}%]')\n","    test_data = load_dataset(\"csv\", data_files=val_data_path, split=f'train[:{split}%]')\n","\n","    # train_data= train_data.shard(num_shards=20000, index=0)\n","    # test_data= test_data.shard(num_shards=500, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","    tokenized_test = test_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics(predictions, labels, class_num=num_labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        fp16=True,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        remove_unused_columns=False,\n","        label_names=[\"labels\"],\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","\n","if __name__ == \"__main__\":\n","\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"4277ef28","metadata":{"id":"4277ef28"},"outputs":[],"source":["import os\n","from typing import List\n","import pandas as pd\n","from datetime import datetime\n","\n","\n","import fire\n","import torch\n","from datasets import load_dataset, Dataset\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import LlamaTokenizer, LlamaForSequenceClassification\n","\n","from torch.nn import CrossEntropyLoss\n","from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n","from utils.eval_utils import cls_metrics_multi\n","\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n","    TaskType\n",")\n","import torch\n","import json\n","import numpy as np\n","\n","from utils.gen_utils import create_folder\n","\n","with open('paths.json', 'r') as f:\n","        path = json.load(f)\n","        multi_train_set_path = path[\"multi_train_set_path\"]\n","        multi_test_set_path = path[\"multi_test_set_path\"]\n","        catche_path = path[\"catche_path\"]\n","        output_path = path[\"output_path\"]\n","        drg_34_dissection_path = path[\"drg_34_dissection_path\"]\n","\n","def train(\n","    base_model: str = \"decapoda-research/llama-7b-hf\",  # the only required argument\n","    model_size: str = \"7b\",\n","    train_data_path: str = multi_train_set_path,\n","    val_data_path: str = multi_test_set_path,\n","    drg_mapping_path: str = drg_34_dissection_path,\n","    cache_dir: str = catche_path,\n","    split: int = 100,\n","    micro_batch_size: int = 4,\n","    num_epochs: int = 3,\n","    learning_rate: float = 2e-5,# 3e-4 is the learning rate used in the LLaMA paper\n","    cutoff_len: int = 512, # consider changing to 1024\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"score\"\n","    ],\n","    padding_side: str = \"right\",\n","    wandb_project: str = \"multilabel-classification\", #other options: \"generative\", \"multilabel-classification\",\n","    wandb_watch: str = \"gradients\",  # options: false | gradients | all ; issues when using all: I have since bypassed this issue by only logging gradient and instead of all.\n","    wandb_log_model: str = \"\",  # options: false | true\n","    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n","):\n","\n","    now = datetime.now()\n","    date_string = now.strftime(\"%B-%d-%H-%M\")\n","    wandb_run_name = f\"{model_size}-{cutoff_len}-{micro_batch_size}-{learning_rate}-{padding_side}-{date_string}\"\n","    output_dir = create_folder(f'{output_path}/{wandb_project}', wandb_run_name)\n","\n","    # load file from train_data_path and find out the unique number of labels\n","    num_labels_pc = pd.read_csv(drg_mapping_path).principal_diagnosis_lable.nunique()\n","    num_labels_cc = pd.read_csv(drg_mapping_path)[\"CC/MCC\"].nunique()\n","    num_labels = num_labels_pc + num_labels_cc\n","\n","    train_data = pd.read_csv(train_data_path, converters={\"label\": lambda x: np.fromstring(x[1:-1], dtype=float, sep=\" \")})\n","    test_data = pd.read_csv(val_data_path, converters={\"label\": lambda x: np.fromstring(x[1:-1], dtype=float, sep=\" \")})\n","\n","    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n","        print(\n","            f\"Training LLaMA-LoRA model with params:\\n\"\n","            f\"base_model: {base_model}\\n\"\n","            f\"model_size: {model_size}\\n\"\n","            f\"train_data_path: {train_data_path}\\n\"\n","            f\"val_data_path: {val_data_path}\\n\"\n","            f\"output_dir: {output_dir}\\n\"\n","            f\"cache_dir: {cache_dir}\\n\"\n","            f\"micro_batch_size: {micro_batch_size}\\n\"\n","            f\"split: {split}\\n\"\n","            f\"num_labels_pc: {num_labels_pc}\\n\"\n","            f\"num_labels_cc: {num_labels_cc}\\n\"\n","            f\"num_labels: {num_labels}\\n\"\n","            f\"num_epochs: {num_epochs}\\n\"\n","            f\"num_train_data: {len(train_data)}\\n\"\n","            f\"num_test_data: {len(test_data)}\\n\"\n","            f\"learning_rate: {learning_rate}\\n\"\n","            f\"cutoff_len: {cutoff_len}\\n\"\n","            f\"lora_r: {lora_r}\\n\"\n","            f\"lora_alpha: {lora_alpha}\\n\"\n","            f\"lora_dropout: {lora_dropout}\\n\"\n","            f\"lora_target_modules: {lora_target_modules}\\n\"\n","            f\"padding_side: {padding_side}\\n\"\n","            f\"wandb_project: {wandb_project}\\n\"\n","            f\"wandb_run_name: {wandb_run_name}\\n\"\n","            f\"wandb_watch: {wandb_watch}\\n\"\n","            f\"wandb_log_model: {wandb_log_model}\\n\"\n","            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n","        )\n","    assert (\n","        base_model\n","    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n","\n","\n","    device_map = \"auto\"\n","    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","    ddp = world_size != 1\n","    if ddp:\n","        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","\n","    # Check if parameter passed or if set within environ\n","    use_wandb = len(wandb_project) > 0 or (\n","        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n","    )\n","    # Only overwrite environ if wandb param passed\n","    if len(wandb_project) > 0:\n","        os.environ[\"WANDB_PROJECT\"] = wandb_project\n","    if len(wandb_watch) > 0:\n","        os.environ[\"WANDB_WATCH\"] = wandb_watch\n","    if len(wandb_log_model) > 0:\n","        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n","\n","\n","    class LlamaForMultilabelSequenceClassification(LlamaForSequenceClassification):\n","        def __init__(self, config):\n","            super().__init__(config)\n","\n","        def forward(self,\n","            input_ids=None,\n","            attention_mask=None,\n","            position_ids=None,\n","            past_key_values=None,\n","            inputs_embeds=None,\n","            labels=None,\n","            use_cache = None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None):\n","            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","            transformer_outputs = self.model(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                position_ids=position_ids,\n","                past_key_values=past_key_values,\n","                inputs_embeds=inputs_embeds,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","            hidden_states = transformer_outputs[0]\n","            logits = self.score(hidden_states)\n","\n","            if input_ids is not None:\n","                batch_size = input_ids.shape[0]\n","            else:\n","                batch_size = inputs_embeds.shape[0]\n","\n","            if self.config.pad_token_id is None and batch_size != 1:\n","                raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n","            if self.config.pad_token_id is None:\n","                sequence_lengths = -1\n","            else:\n","                if input_ids is not None:\n","                    sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n","                else:\n","                    sequence_lengths = -1\n","\n","            pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n","\n","            loss = None\n","            if labels is not None:\n","                labels = labels.to(logits.device)\n","                loss_fct_pc = CrossEntropyLoss()\n","                loss_fct_cc = CrossEntropyLoss()\n","\n","                logits_pc = pooled_logits[:, :num_labels_pc]\n","                labels_onehot_pc = labels[:, :num_labels_pc]\n","                labels_pc = torch.argmax(labels_onehot_pc, axis=1)\n","\n","                logits_cc = pooled_logits[:, num_labels_pc:]\n","                labels_onehot_cc = labels[:, num_labels_pc:]\n","                labels_cc = torch.argmax(labels_onehot_cc, axis=1)\n","\n","                loss_pc = loss_fct_pc(logits_pc, labels_pc)\n","                loss_cc = loss_fct_cc(logits_cc, labels_cc)\n","                loss = loss_pc + 0.5*loss_cc\n","            if not return_dict:\n","                output = (pooled_logits,) + transformer_outputs[1:]\n","                return ((loss,) + output) if loss is not None else output\n","\n","            return SequenceClassifierOutputWithPast(\n","                loss=loss,\n","                logits=pooled_logits,\n","                past_key_values=transformer_outputs.past_key_values,\n","                hidden_states=transformer_outputs.hidden_states,\n","                attentions=transformer_outputs.attentions,\n","            )\n","\n","\n","    model = LlamaForMultilabelSequenceClassification.from_pretrained(\n","        base_model,\n","        num_labels=num_labels,\n","        load_in_8bit=True,\n","        problem_type=\"multi_label_classification\",\n","        torch_dtype=torch.float16,\n","        device_map=device_map,\n","        cache_dir=cache_dir)\n","\n","    tokenizer = LlamaTokenizer.from_pretrained(\n","        base_model,\n","        model_max_length=cutoff_len,\n","        cache_dir=cache_dir)\n","\n","    # This is to fix the bad token in \"decapoda-research/llama-7b-hf\"\n","    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n","    model.config.bos_token_id = 1\n","    model.config.eos_token_id = 2\n","\n","    model = prepare_model_for_int8_training(model)\n","\n","    config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        target_modules=lora_target_modules,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=TaskType.SEQ_CLS,\n","        modules_to_save=None\n","    )\n","    model = get_peft_model(model, config)\n","\n","    if resume_from_checkpoint:\n","        # Check the available weights and load them\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"pytorch_model.bin\"\n","        )  # Full checkpoint\n","        if not os.path.exists(checkpoint_name):\n","            checkpoint_name = os.path.join(\n","                resume_from_checkpoint, \"adapter_model.bin\"\n","            )  # only LoRA model - LoRA config above has to fit\n","            resume_from_checkpoint = (\n","                False  # So the trainer won't try loading its state\n","            )\n","        # The two files above have a different name depending on how they were saved, but are actually the same.\n","        if os.path.exists(checkpoint_name):\n","            print(f\"Restarting from {checkpoint_name}\")\n","            adapters_weights = torch.load(checkpoint_name)\n","            set_peft_model_state_dict(model, adapters_weights)\n","        else:\n","            print(f\"Checkpoint {checkpoint_name} not found\")\n","\n","    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name, param.shape)\n","\n","    def preprocess_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True)\n","\n","    train_data = Dataset.from_pandas(train_data)\n","    test_data = Dataset.from_pandas(test_data)\n","\n","    # train_data= train_data.shard(num_shards=5000, index=0)\n","    # test_data= test_data.shard(num_shards=5000, index=0)\n","\n","    tokenized_train = train_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","    tokenized_test = test_data.map(preprocess_function, batched=True).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n","\n","    # default is padding to longest\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    def compute_metrics_multi(eval_pred):\n","        predictions, labels = eval_pred\n","        return cls_metrics_multi(y_pred=predictions, y=labels)\n","\n","    # Other hyperparameters to consider here is gradient_accumulation_steps, weight decay, learning rate, adam etype\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=micro_batch_size,\n","        per_device_eval_batch_size=micro_batch_size,\n","        num_train_epochs=num_epochs,\n","        weight_decay=0.01,\n","        fp16=True,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        push_to_hub=False,\n","        remove_unused_columns=False,\n","        label_names=[\"labels\"],\n","        ddp_find_unused_parameters=False if ddp else None,\n","        report_to=\"wandb\" if use_wandb else None,\n","        run_name=wandb_run_name if use_wandb else None,\n","        )\n","\n","    if not ddp and torch.cuda.device_count() > 1:\n","        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","        model.is_parallelizable = True\n","        model.model_parallel = True\n","\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_test,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics_multi,\n","        )\n","\n","    model.config.use_cache = False\n","\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n","\n","    model.save_pretrained(output_dir)\n","\n","if __name__ == \"__main__\":\n","    fire.Fire(train)"]},{"cell_type":"code","execution_count":null,"id":"ef33fa42","metadata":{"id":"ef33fa42"},"outputs":[],"source":["# Adopted from https://github.com/JHLiu7/EarlyDRGPrediction\n","\n","\n","import math\n","import pandas as pd\n","import numpy as np\n","import pickle as pk\n","import json\n","\n","\n","from sklearn.metrics import auc, roc_curve, precision_recall_curve, f1_score, accuracy_score\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from scipy.stats import pearsonr, spearmanr\n","\n","with open('paths.json', 'r') as f:\n","    path = json.load(f)\n","    drg_34_dissection_path = path[\"drg_34_dissection_path\"]\n","\n","# read a csv file but only read in the column of principal_diagnosis_lable and CC/MCC\n","pc_cc_mapping = pd.read_csv(drg_34_dissection_path)[[\"principal_diagnosis_lable\", \"CC/MCC\"]]\n","num_labels_pc = pc_cc_mapping.principal_diagnosis_lable.nunique()\n","\n","# make a dictinoary where key is principal_diagnosis_lable and value is CC/MCC. For one key there can be multiple values\n","pc_cc_dict = {}\n","for index, row in pc_cc_mapping.iterrows():\n","    if row[\"principal_diagnosis_lable\"] not in pc_cc_dict:\n","        pc_cc_dict[row[\"principal_diagnosis_lable\"]] = [row[\"CC/MCC\"]]\n","    else:\n","        pc_cc_dict[row[\"principal_diagnosis_lable\"]].append(row[\"CC/MCC\"])\n","\n","\n","\n","def map_rule(pred_pc, label_pc, pred_cc, label_cc):\n","    if pred_pc == label_pc:\n","        if pred_cc == label_cc:\n","            return True\n","        # if there's only one cc/MCC code of this principal diagnosis code, then any predcitons would be right\n","        elif len(pc_cc_dict[label_pc]) == 1:\n","            return True\n","        elif pred_cc in pc_cc_dict[label_pc] and pred_cc != label_cc:\n","            return False\n","        # for this group, default is to group 0\n","        elif set(pc_cc_dict[label_pc]) == {2, 1, 0}:\n","            mark_cc = 0\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","        # for this group, default is to group 3: without MCC\n","        elif set(pc_cc_dict[label_pc]) == {2, 3}:\n","            mark_cc = 3\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","        # for this group, there are two scenario. With MCC wil default to group 1, others will default to group 0\n","        elif set(pc_cc_dict[label_pc]) == {1, 0}:\n","            if pred_cc == 2:\n","                mark_cc = 1\n","            else:\n","                mark_cc = 0\n","            if mark_cc == label_cc:\n","                return True\n","            else:\n","                return False\n","    return False\n","\n","map_rule(12,12,1,2)\n","\n","\n","def accuracies_map(y_pred_pc, labels_pc, y_pred_cc, labels_cc):\n","    acc = 0.0\n","    num = len(y_pred_pc)\n","\n","    for i in range(num):\n","        if map_rule(y_pred_pc[i], labels_pc[i], y_pred_cc[i], labels_cc[i]):\n","            acc += 1.\n","    acc /= num\n","\n","    return acc\n","\n","\n","# full evaluation\n","def full_metrics(y_pred, y, drg_rule, d2i):\n","    y_pred_w, y_w = map2weight(y_pred, y, drg_rule=drg_rule, d2i=d2i)\n","\n","    reg_dict = reg_metrics(y_pred_w, y_w)\n","\n","    full_dict = {}\n","    full_dict.update(reg_dict)\n","\n","    cls_dict = cls_metrics(y_pred, y, len(d2i))\n","    full_dict.update(cls_dict)\n","\n","    return full_dict\n","\n","def cls_metrics(y_pred, y, class_num):\n","    # class_num = args.Y\n","    y_pred_ = softmax(y_pred)\n","    y_ = onehot_encode(y, class_num)\n","\n","    macroAUC, microAUC, appeared, cases = ave_auc_scores(y_pred_, y_)\n","    macroF1, microF1 = ave_f1_scores(y_pred, y)\n","\n","    metric_dict = {\n","        'microF1':microF1, 'macroF1':macroF1,\n","        'microAUC':microAUC, 'macroAUC':macroAUC,\n","        'labels': appeared, 'count': cases\n","    }\n","\n","    metric_dict['acc10'], metric_dict['acc5'], metric_dict['acc'], _ = accuracies(y_pred, y)\n","    return metric_dict\n","\n","def cls_metrics_eval(y_pred, y, class_num):\n","    # class_num = args.Y\n","    y_pred_ = softmax(y_pred)\n","    y_ = onehot_encode(y, class_num)\n","\n","    macroAUC, microAUC, appeared, cases = ave_auc_scores(y_pred_, y_)\n","    macroF1, microF1 = ave_f1_scores(y_pred, y)\n","\n","    metric_dict = {\n","        'microF1':microF1, 'macroF1':macroF1,\n","        'microAUC':microAUC, 'macroAUC':macroAUC,\n","        'labels': appeared, 'count': cases\n","    }\n","\n","    metric_dict['acc10'], metric_dict['acc5'], metric_dict['acc'], _ = accuracies(y_pred, y)\n","    metric_dict['y_label'] = y\n","    metric_dict['y_raw'] = y_pred\n","    # https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n","    metric_dict['y_raw_top5'] = (-y_pred).argsort(axis=1)[:, :5]\n","    metric_dict['y_pred'] = np.argmax(y_pred_, axis=1)\n","\n","    return metric_dict\n","\n","def cls_metrics_multi(y_pred, y):\n","    # class_num = args.Y\n","    predictions_pc = y_pred[:, :num_labels_pc]\n","    y_pred_pc = softmax(predictions_pc)\n","    labels_onehot_pc = y[:, :num_labels_pc]\n","    labels_pc = np.argmax(labels_onehot_pc, axis=1)\n","    predictions_cc = y_pred[:, num_labels_pc:]\n","    y_pred_cc = softmax(predictions_cc)\n","    labels_onehot_cc = y[:, num_labels_pc:]\n","    labels_cc = np.argmax(labels_onehot_cc, axis=1)\n","\n","    ## Need to double check it mirrows the original methods\n","\n","    macroAUC_pc, microAUC_pc, appeared_pc, cases_pc = ave_auc_scores(y_pred_pc, labels_onehot_pc)\n","    macroF1_pc, microF1_pc = ave_f1_scores(predictions_pc, labels_pc)\n","    acc10_pc, acc5_pc, acc_pc, _ = accuracies(predictions_pc, labels_pc)\n","\n","    macroAUC_cc, microAUC_cc, appeared_cc, cases_cc = ave_auc_scores(y_pred_cc, labels_onehot_cc)\n","    macroF1_cc, microF1_cc = ave_f1_scores(predictions_cc, labels_cc)\n","    acc10_cc, acc5_cc, acc_cc, _ = accuracies(predictions_cc, labels_cc)\n","\n","    y_pred_pc_single = np.argmax(y_pred_pc, axis=1)\n","    y_pred_cc_single = np.argmax(y_pred_cc, axis=1)\n","\n","    acc_map = accuracies_map(y_pred_pc_single, labels_pc, y_pred_cc_single, labels_cc)\n","\n","    metric_dict = {\n","        'acc_map': acc_map,\n","        'microF1_pc':microF1_pc, 'macroF1_pc':macroF1_pc,\n","        'microAUC_pc':microAUC_pc, 'macroAUC_pc':macroAUC_pc,\n","        'labels_pc': appeared_pc, 'count_pc': cases_pc,\n","        'acc10_pc': acc10_pc, 'acc5_pc': acc5_pc, 'acc_pc': acc_pc,\n","        'microF1_cc':microF1_cc, 'macroF1_cc':macroF1_cc,\n","        'microAUC_cc':microAUC_cc, 'macroAUC_cc':macroAUC_cc,\n","        'labels_cc': appeared_cc, 'count_cc': cases_cc,\n","        'acc10_cc': acc10_cc, 'acc5_cc': acc5_cc, 'acc_cc': acc_cc\n","    }\n","\n","    return metric_dict\n","\n","def reg_metrics(y_pred, y):\n","    mae = mean_absolute_error(y_pred, y)\n","    mse = mean_squared_error(y_pred,  y)\n","    spearman, p = spearmanr(y_pred, y)\n","\n","    metric_dict = {\n","        'MAE': mae, 'MSE': mse, 'RMSE': math.sqrt(mse),\n","        'spearman': spearman, 'corr_p': p\n","    }\n","\n","    dist= y_pred - y\n","    cmi = np.mean(dist)\n","    overshot, undershot = len(dist[dist>0]), len(dist[dist<0])\n","\n","    metric_dict.update({\n","        'CMI_error': cmi/np.mean(y), 'CMI_raw':cmi, 'overshot': overshot, 'undershot': undershot\n","    })\n","    return metric_dict\n","\n","def cls_metrics_multi_eval(y_pred, y):\n","    # class_num = args.Y\n","\n","    predictions_pc = y_pred[:, :num_labels_pc]\n","    y_pred_pc = softmax(predictions_pc)\n","    labels_onehot_pc = y[:, :num_labels_pc]\n","    labels_pc = np.argmax(labels_onehot_pc, axis=1)\n","    predictions_cc = y_pred[:, num_labels_pc:]\n","    y_pred_cc = softmax(predictions_cc)\n","    labels_onehot_cc = y[:, num_labels_pc:]\n","    labels_cc = np.argmax(labels_onehot_cc, axis=1)\n","\n","    ## Need to double check it mirrows the original methods\n","\n","    macroAUC_pc, microAUC_pc, appeared_pc, cases_pc = ave_auc_scores(y_pred_pc, labels_onehot_pc)\n","    macroF1_pc, microF1_pc = ave_f1_scores(predictions_pc, labels_pc)\n","    acc10_pc, acc5_pc, acc_pc, _ = accuracies(predictions_pc, labels_pc)\n","\n","    macroAUC_cc, microAUC_cc, appeared_cc, cases_cc = ave_auc_scores(y_pred_cc, labels_onehot_cc)\n","    macroF1_cc, microF1_cc = ave_f1_scores(predictions_cc, labels_cc)\n","    acc10_cc, acc5_cc, acc_cc, _ = accuracies(predictions_cc, labels_cc)\n","\n","    y_pred_pc_single = np.argmax(y_pred_pc, axis=1)\n","    y_pred_cc_single = np.argmax(y_pred_cc, axis=1)\n","\n","    acc_map = accuracies_map(y_pred_pc_single, labels_pc, y_pred_cc_single, labels_cc)\n","\n","    metric_dict = {\n","        'acc_map': acc_map,\n","        'microF1_pc':microF1_pc, 'macroF1_pc':macroF1_pc,\n","        'microAUC_pc':microAUC_pc, 'macroAUC_pc':macroAUC_pc,\n","        'labels_pc': appeared_pc, 'count_pc': cases_pc,\n","        'acc10_pc': acc10_pc, 'acc5_pc': acc5_pc, 'acc_pc': acc_pc,\n","        'microF1_cc':microF1_cc, 'macroF1_cc':macroF1_cc,\n","        'microAUC_cc':microAUC_cc, 'macroAUC_cc':macroAUC_cc,\n","        'labels_cc': appeared_cc, 'count_cc': cases_cc,\n","        'acc10_cc': acc10_cc, 'acc5_cc': acc5_cc, 'acc_cc': acc_cc\n","    }\n","\n","    metric_dict['y_label'] = y\n","    metric_dict['y_raw'] = y_pred\n","\n","    return metric_dict\n","\n","\n","# to print out results\n","def result2str(d):\n","    try:\n","        mif, maf = d['microF1'], d['macroF1']\n","        mia, maa = d['microAUC'], d['macroAUC']\n","        a10, a5, a = d['acc10'], d['acc5'], d['acc']\n","        la, ct = d['labels'], d['count']\n","    except:\n","        pass\n","    ma, rm = d['MAE'], d['RMSE']\n","    sp, p = d['spearman'], d['corr_p']\n","    cm,ov,ud = d['CMI_error'], d['overshot'], d['undershot']\n","\n","    title = \"****\" * 5 + '\\n'\n","    try:\n","        s1 = \"{} cases, {} labels\".format(ct, la)\n","        s2 = \"MACRO-AUC     \\tMICRO-AUC      \\tMACRO-F1     \\tMICRO-F1  \"\n","        s3 = \"{:.4f}  \\t{:.4f}  \\t{:.4f}  \\t{:.4f}\".format(maa, mia, maf, mif)\n","        s4 = \"Acc10  \\tAcc5  \\tAcc \"\n","        s5 = \"{:.4f}  \\t{:.4f}  \\t{:.4f}  \\n\".format(a10, a5, a)\n","        title = title+'\\n'.join([s1, s2, s3, s4, s5])\n","    except:\n","        pass\n","    r1 = \"MAE: {:.4f}  RMSE: {:.4f}  Corr: {:.4f}  \\n\".format(ma, rm, sp)\n","    r2 = \"CMI_error: {:.2%}  overshot: {}  undershot: {}  \\n\\n\".format(cm,ov,ud)\n","\n","    title = title+'\\n'.join([r1, r2])\n","\n","    return title\n","\n","\n","# running evaluation\n","def score_f1(y_pred, y):\n","    \"\"\"\n","        y_pred: logit\n","    \"\"\"\n","    y_flat = np.argmax(y_pred, axis=1)\n","    return f1_score(y, y_flat, average='micro')\n","\n","def score_mae(y_pred, y):\n","    return mean_absolute_error(y_pred, y)\n","\n","\n","# utils\n","def map2weight(y_pred, y, drg_rule, d2i):\n","\n","    idx2drg = {v:k for k,v in d2i.items()}\n","    drg2weight = {}\n","    for _, row in drg_rule.iterrows():\n","        drg2weight[row['DRG_CODE']] = row['WEIGHT']\n","\n","    y_pred = [drg2weight[idx2drg[d]] for d in np.argmax(y_pred, axis=1)]\n","    y = [drg2weight[idx2drg[d]] for d in y]\n","    return np.array(y_pred), np.array(y)\n","\n","def softmax(x):\n","    e_x = np.exp(x)\n","    return e_x / np.expand_dims(e_x.sum(axis=1), 1)\n","\n","def onehot_encode(y, class_num):\n","    \"\"\"\n","        y: a flat array of labels\n","    \"\"\"\n","    yone = []\n","    for i in y:\n","        onehot = np.zeros(class_num)\n","        onehot[i] = 1\n","        yone.append(onehot)\n","    return np.array(yone)\n","\n","def accuracies(y_pred, y, onlyAcc=False):\n","    \"\"\"\n","    y_pred: logits\n","    y: a list of labels\n","    \"\"\"\n","    acc10 = 0.0\n","    acc5 = 0.0\n","    acc1 = 0.0\n","    num = len(y)\n","\n","    for i in range(num):\n","\n","        pred = y_pred[i]\n","        top10_pred = set(pred.argsort()[-10:])\n","        top5_pred = set(pred.argsort()[-5:])\n","        top1_pred = set(pred.argsort()[-1:])\n","\n","        label = y[i]\n","        # label = np.argmax(y[i])\n","\n","        if label in top10_pred:\n","            acc10 += 1.\n","        if label in top5_pred:\n","            acc5 += 1.\n","        if label in top1_pred:\n","            acc1 += 1.\n","\n","    acc10 /= num\n","    acc5 /= num\n","    acc1 /= num\n","\n","    if onlyAcc:\n","        return acc1\n","    return acc10, acc5, acc1, num\n","\n","def ave_auc_scores(y_pred, y):\n","    # micro/macro auc based on classes\n","    \"\"\"\n","        y.shape: [sample, classes] float\n","        y_pred.shape: [sample, classes] int\n","        numpy\n","    \"\"\"\n","\n","    aucroc_cases = {}\n","    for i in range(y.shape[1]):\n","        if y[:, i].sum()>0: # class appears in test set\n","            fp, tp, _ = roc_curve(y[:, i], y_pred[:, i])\n","            if len(fp) >1 and len(tp) >1:\n","                auc_roc = auc(fp, tp)\n","                aucroc_cases[i] = auc_roc\n","\n","    fp_mic, tp_mic, _ = roc_curve(y.ravel(), y_pred.ravel())\n","\n","    # appearing classes\n","    labels = list(aucroc_cases.keys())\n","\n","    # roc\n","    auc_roc_macro = np.mean(list(aucroc_cases.values()))\n","    auc_roc_micro = auc(fp_mic, tp_mic)\n","    return auc_roc_macro, auc_roc_micro, len(labels), len(y)\n","\n","def ave_f1_scores(y_pred, y):\n","    # f1\n","    # require y_pred, y being flat list\n","    y_flat = np.argmax(y_pred, axis=1)\n","\n","    f1_macro = f1_score(y, y_flat, average='macro', labels=np.unique(y))\n","    f1_micro = f1_score(y, y_flat, average='micro', labels=np.unique(y))\n","\n","    return f1_macro, f1_micro\n"]},{"cell_type":"code","execution_count":null,"id":"1f405343","metadata":{"id":"1f405343"},"outputs":[],"source":["import os\n","\n","def create_folder(parent_path, folder):\n","    if not parent_path.endswith('/'):\n","        parent_path += '/'\n","    folder_path = parent_path + folder\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","    return folder_path"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}