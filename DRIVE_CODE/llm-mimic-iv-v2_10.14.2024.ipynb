{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers[torch] in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (4.41.2)\n","Requirement already satisfied: filelock in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\n","Requirement already satisfied: requests in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from transformers[torch]) (2.2.0)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: psutil in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\n","Requirement already satisfied: sympy in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch->transformers[torch]) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.3.101)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->transformers[torch]) (2022.9.24)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n","Requirement already satisfied: mpmath>=0.19 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: accelerate\n","Successfully installed accelerate-0.33.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: accelerate in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (0.33.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (2.2.0)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.12.2)\n","Requirement already satisfied: requests in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.9.24)\n","Requirement already satisfied: mpmath>=0.19 in /home/iqh4001/.conda/envs/jupyter-notebook/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["\"\"\"\n","!pip install transformers[torch]\n","!pip install accelerate -U\n","\"\"\""]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-28T20:51:57.127409Z","iopub.status.busy":"2024-07-28T20:51:57.127032Z","iopub.status.idle":"2024-07-28T20:52:59.465444Z","shell.execute_reply":"2024-07-28T20:52:59.464606Z","shell.execute_reply.started":"2024-07-28T20:51:57.127378Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from tqdm import tqdm\n","import shap\n","import joblib\n","import xgboost as xgb\n","\"\"\"\n","# Load data\n","data_discharge_notes = pd.read_csv(\"/kaggle/input/mimic-iv-prime/discharge.csv\")\n","data_mimic_patients = pd.read_csv('/kaggle/input/mimic-iv-prime/patients.csv')\n","\"\"\"\n","\n","data_mimic_patients = pd.read_csv('/home/iqh4001/Iqram_WCM/MADE Lab/LLM_EHR/Data/mimic-iv-2.2/hosp/patients.csv')\n","\n","data_discharge_notes = pd.read_csv('/home/iqh4001/Iqram_WCM/MADE Lab/LLM_EHR/Data/MIMIC-IV Data/discharge.csv')\n","\n","#data_discharge = pd.read_csv('/home/iqh4001/Iqram_WCM/MADE Lab/LLM_EHR/Data/MIMIC-IV Data/discharge_detail.csv')\n","\n","# Merge data_discharge_notes with data_mimic_patients on 'subject_id'\n","data_discharge_notes_1 = data_discharge_notes.merge(data_mimic_patients[['subject_id', 'dod']], on='subject_id', how='left')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T20:52:59.467316Z","iopub.status.busy":"2024-07-28T20:52:59.467025Z","iopub.status.idle":"2024-07-28T20:52:59.487873Z","shell.execute_reply":"2024-07-28T20:52:59.486992Z","shell.execute_reply.started":"2024-07-28T20:52:59.467291Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>note_id</th>\n","      <th>subject_id</th>\n","      <th>hadm_id</th>\n","      <th>note_type</th>\n","      <th>note_seq</th>\n","      <th>charttime</th>\n","      <th>storetime</th>\n","      <th>text</th>\n","      <th>dod</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10000032-DS-21</td>\n","      <td>10000032</td>\n","      <td>22595853</td>\n","      <td>DS</td>\n","      <td>21</td>\n","      <td>2180-05-07 00:00:00</td>\n","      <td>2180-05-09 15:26:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10000032-DS-22</td>\n","      <td>10000032</td>\n","      <td>22841357</td>\n","      <td>DS</td>\n","      <td>22</td>\n","      <td>2180-06-27 00:00:00</td>\n","      <td>2180-07-01 10:15:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10000032-DS-23</td>\n","      <td>10000032</td>\n","      <td>29079034</td>\n","      <td>DS</td>\n","      <td>23</td>\n","      <td>2180-07-25 00:00:00</td>\n","      <td>2180-07-25 21:42:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10000032-DS-24</td>\n","      <td>10000032</td>\n","      <td>25742920</td>\n","      <td>DS</td>\n","      <td>24</td>\n","      <td>2180-08-07 00:00:00</td>\n","      <td>2180-08-10 05:43:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10000084-DS-17</td>\n","      <td>10000084</td>\n","      <td>23052089</td>\n","      <td>DS</td>\n","      <td>17</td>\n","      <td>2160-11-25 00:00:00</td>\n","      <td>2160-11-25 15:09:00</td>\n","      <td>\\nName:  ___                    Unit No:   __...</td>\n","      <td>2161-02-13</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>331788</th>\n","      <td>19999828-DS-6</td>\n","      <td>19999828</td>\n","      <td>29734428</td>\n","      <td>DS</td>\n","      <td>6</td>\n","      <td>2147-08-04 00:00:00</td>\n","      <td>2147-08-12 15:36:00</td>\n","      <td>\\nName:  ___                   Unit No:   ___...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>331789</th>\n","      <td>19999828-DS-7</td>\n","      <td>19999828</td>\n","      <td>25744818</td>\n","      <td>DS</td>\n","      <td>7</td>\n","      <td>2149-01-18 00:00:00</td>\n","      <td>2149-01-19 07:03:00</td>\n","      <td>\\nName:  ___                   Unit No:   ___...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>331790</th>\n","      <td>19999840-DS-20</td>\n","      <td>19999840</td>\n","      <td>26071774</td>\n","      <td>DS</td>\n","      <td>20</td>\n","      <td>2164-07-28 00:00:00</td>\n","      <td>2164-07-29 14:52:00</td>\n","      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n","      <td>2164-09-17</td>\n","    </tr>\n","    <tr>\n","      <th>331791</th>\n","      <td>19999840-DS-21</td>\n","      <td>19999840</td>\n","      <td>21033226</td>\n","      <td>DS</td>\n","      <td>21</td>\n","      <td>2164-09-17 00:00:00</td>\n","      <td>2164-09-18 01:36:00</td>\n","      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n","      <td>2164-09-17</td>\n","    </tr>\n","    <tr>\n","      <th>331792</th>\n","      <td>19999987-DS-2</td>\n","      <td>19999987</td>\n","      <td>23865745</td>\n","      <td>DS</td>\n","      <td>2</td>\n","      <td>2145-11-11 00:00:00</td>\n","      <td>2145-11-11 13:13:00</td>\n","      <td>\\nName:  ___                    Unit No:   __...</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>331793 rows × 9 columns</p>\n","</div>"],"text/plain":["               note_id  subject_id   hadm_id note_type  note_seq  \\\n","0       10000032-DS-21    10000032  22595853        DS        21   \n","1       10000032-DS-22    10000032  22841357        DS        22   \n","2       10000032-DS-23    10000032  29079034        DS        23   \n","3       10000032-DS-24    10000032  25742920        DS        24   \n","4       10000084-DS-17    10000084  23052089        DS        17   \n","...                ...         ...       ...       ...       ...   \n","331788   19999828-DS-6    19999828  29734428        DS         6   \n","331789   19999828-DS-7    19999828  25744818        DS         7   \n","331790  19999840-DS-20    19999840  26071774        DS        20   \n","331791  19999840-DS-21    19999840  21033226        DS        21   \n","331792   19999987-DS-2    19999987  23865745        DS         2   \n","\n","                  charttime            storetime  \\\n","0       2180-05-07 00:00:00  2180-05-09 15:26:00   \n","1       2180-06-27 00:00:00  2180-07-01 10:15:00   \n","2       2180-07-25 00:00:00  2180-07-25 21:42:00   \n","3       2180-08-07 00:00:00  2180-08-10 05:43:00   \n","4       2160-11-25 00:00:00  2160-11-25 15:09:00   \n","...                     ...                  ...   \n","331788  2147-08-04 00:00:00  2147-08-12 15:36:00   \n","331789  2149-01-18 00:00:00  2149-01-19 07:03:00   \n","331790  2164-07-28 00:00:00  2164-07-29 14:52:00   \n","331791  2164-09-17 00:00:00  2164-09-18 01:36:00   \n","331792  2145-11-11 00:00:00  2145-11-11 13:13:00   \n","\n","                                                     text         dod  \n","0        \\nName:  ___                     Unit No:   _...  2180-09-09  \n","1        \\nName:  ___                     Unit No:   _...  2180-09-09  \n","2        \\nName:  ___                     Unit No:   _...  2180-09-09  \n","3        \\nName:  ___                     Unit No:   _...  2180-09-09  \n","4        \\nName:  ___                    Unit No:   __...  2161-02-13  \n","...                                                   ...         ...  \n","331788   \\nName:  ___                   Unit No:   ___...         NaN  \n","331789   \\nName:  ___                   Unit No:   ___...         NaN  \n","331790   \\nName:  ___                  Unit No:   ___\\...  2164-09-17  \n","331791   \\nName:  ___                  Unit No:   ___\\...  2164-09-17  \n","331792   \\nName:  ___                    Unit No:   __...         NaN  \n","\n","[331793 rows x 9 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data_discharge_notes_1"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T20:52:59.489111Z","iopub.status.busy":"2024-07-28T20:52:59.488864Z","iopub.status.idle":"2024-07-28T20:52:59.695449Z","shell.execute_reply":"2024-07-28T20:52:59.694649Z","shell.execute_reply.started":"2024-07-28T20:52:59.489090Z"},"trusted":true},"outputs":[],"source":["# Convert 'charttime' and 'dod' to datetime format\n","data_discharge_notes_1['charttime'] = pd.to_datetime(data_discharge_notes_1['charttime'])\n","data_discharge_notes_1['dod'] = pd.to_datetime(data_discharge_notes_1['dod'])\n","\n","# Calculate the difference in days between 'dod' and 'charttime'\n","data_discharge_notes_1['Out_of_hospital_mortality_days'] = (data_discharge_notes_1['dod'] - data_discharge_notes_1['charttime']).dt.days\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T20:52:59.698189Z","iopub.status.busy":"2024-07-28T20:52:59.697837Z","iopub.status.idle":"2024-07-28T20:52:59.718015Z","shell.execute_reply":"2024-07-28T20:52:59.716986Z","shell.execute_reply.started":"2024-07-28T20:52:59.698157Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>note_id</th>\n","      <th>subject_id</th>\n","      <th>hadm_id</th>\n","      <th>note_type</th>\n","      <th>note_seq</th>\n","      <th>charttime</th>\n","      <th>storetime</th>\n","      <th>text</th>\n","      <th>dod</th>\n","      <th>Out_of_hospital_mortality_days</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10000032-DS-21</td>\n","      <td>10000032</td>\n","      <td>22595853</td>\n","      <td>DS</td>\n","      <td>21</td>\n","      <td>2180-05-07</td>\n","      <td>2180-05-09 15:26:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","      <td>125.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10000032-DS-22</td>\n","      <td>10000032</td>\n","      <td>22841357</td>\n","      <td>DS</td>\n","      <td>22</td>\n","      <td>2180-06-27</td>\n","      <td>2180-07-01 10:15:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","      <td>74.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10000032-DS-23</td>\n","      <td>10000032</td>\n","      <td>29079034</td>\n","      <td>DS</td>\n","      <td>23</td>\n","      <td>2180-07-25</td>\n","      <td>2180-07-25 21:42:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","      <td>46.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10000032-DS-24</td>\n","      <td>10000032</td>\n","      <td>25742920</td>\n","      <td>DS</td>\n","      <td>24</td>\n","      <td>2180-08-07</td>\n","      <td>2180-08-10 05:43:00</td>\n","      <td>\\nName:  ___                     Unit No:   _...</td>\n","      <td>2180-09-09</td>\n","      <td>33.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10000084-DS-17</td>\n","      <td>10000084</td>\n","      <td>23052089</td>\n","      <td>DS</td>\n","      <td>17</td>\n","      <td>2160-11-25</td>\n","      <td>2160-11-25 15:09:00</td>\n","      <td>\\nName:  ___                    Unit No:   __...</td>\n","      <td>2161-02-13</td>\n","      <td>80.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>331788</th>\n","      <td>19999828-DS-6</td>\n","      <td>19999828</td>\n","      <td>29734428</td>\n","      <td>DS</td>\n","      <td>6</td>\n","      <td>2147-08-04</td>\n","      <td>2147-08-12 15:36:00</td>\n","      <td>\\nName:  ___                   Unit No:   ___...</td>\n","      <td>NaT</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>331789</th>\n","      <td>19999828-DS-7</td>\n","      <td>19999828</td>\n","      <td>25744818</td>\n","      <td>DS</td>\n","      <td>7</td>\n","      <td>2149-01-18</td>\n","      <td>2149-01-19 07:03:00</td>\n","      <td>\\nName:  ___                   Unit No:   ___...</td>\n","      <td>NaT</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>331790</th>\n","      <td>19999840-DS-20</td>\n","      <td>19999840</td>\n","      <td>26071774</td>\n","      <td>DS</td>\n","      <td>20</td>\n","      <td>2164-07-28</td>\n","      <td>2164-07-29 14:52:00</td>\n","      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n","      <td>2164-09-17</td>\n","      <td>51.0</td>\n","    </tr>\n","    <tr>\n","      <th>331791</th>\n","      <td>19999840-DS-21</td>\n","      <td>19999840</td>\n","      <td>21033226</td>\n","      <td>DS</td>\n","      <td>21</td>\n","      <td>2164-09-17</td>\n","      <td>2164-09-18 01:36:00</td>\n","      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n","      <td>2164-09-17</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>331792</th>\n","      <td>19999987-DS-2</td>\n","      <td>19999987</td>\n","      <td>23865745</td>\n","      <td>DS</td>\n","      <td>2</td>\n","      <td>2145-11-11</td>\n","      <td>2145-11-11 13:13:00</td>\n","      <td>\\nName:  ___                    Unit No:   __...</td>\n","      <td>NaT</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>331793 rows × 10 columns</p>\n","</div>"],"text/plain":["               note_id  subject_id   hadm_id note_type  note_seq  charttime  \\\n","0       10000032-DS-21    10000032  22595853        DS        21 2180-05-07   \n","1       10000032-DS-22    10000032  22841357        DS        22 2180-06-27   \n","2       10000032-DS-23    10000032  29079034        DS        23 2180-07-25   \n","3       10000032-DS-24    10000032  25742920        DS        24 2180-08-07   \n","4       10000084-DS-17    10000084  23052089        DS        17 2160-11-25   \n","...                ...         ...       ...       ...       ...        ...   \n","331788   19999828-DS-6    19999828  29734428        DS         6 2147-08-04   \n","331789   19999828-DS-7    19999828  25744818        DS         7 2149-01-18   \n","331790  19999840-DS-20    19999840  26071774        DS        20 2164-07-28   \n","331791  19999840-DS-21    19999840  21033226        DS        21 2164-09-17   \n","331792   19999987-DS-2    19999987  23865745        DS         2 2145-11-11   \n","\n","                  storetime  \\\n","0       2180-05-09 15:26:00   \n","1       2180-07-01 10:15:00   \n","2       2180-07-25 21:42:00   \n","3       2180-08-10 05:43:00   \n","4       2160-11-25 15:09:00   \n","...                     ...   \n","331788  2147-08-12 15:36:00   \n","331789  2149-01-19 07:03:00   \n","331790  2164-07-29 14:52:00   \n","331791  2164-09-18 01:36:00   \n","331792  2145-11-11 13:13:00   \n","\n","                                                     text        dod  \\\n","0        \\nName:  ___                     Unit No:   _... 2180-09-09   \n","1        \\nName:  ___                     Unit No:   _... 2180-09-09   \n","2        \\nName:  ___                     Unit No:   _... 2180-09-09   \n","3        \\nName:  ___                     Unit No:   _... 2180-09-09   \n","4        \\nName:  ___                    Unit No:   __... 2161-02-13   \n","...                                                   ...        ...   \n","331788   \\nName:  ___                   Unit No:   ___...        NaT   \n","331789   \\nName:  ___                   Unit No:   ___...        NaT   \n","331790   \\nName:  ___                  Unit No:   ___\\... 2164-09-17   \n","331791   \\nName:  ___                  Unit No:   ___\\... 2164-09-17   \n","331792   \\nName:  ___                    Unit No:   __...        NaT   \n","\n","        Out_of_hospital_mortality_days  \n","0                                125.0  \n","1                                 74.0  \n","2                                 46.0  \n","3                                 33.0  \n","4                                 80.0  \n","...                                ...  \n","331788                             NaN  \n","331789                             NaN  \n","331790                            51.0  \n","331791                             0.0  \n","331792                             NaN  \n","\n","[331793 rows x 10 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data_discharge_notes_1"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T20:52:59.719694Z","iopub.status.busy":"2024-07-28T20:52:59.719334Z","iopub.status.idle":"2024-07-28T20:52:59.725594Z","shell.execute_reply":"2024-07-28T20:52:59.724653Z","shell.execute_reply.started":"2024-07-28T20:52:59.719660Z"},"trusted":true},"outputs":[],"source":["# Prepare the text data (X) and the target variable (y)\n","X = data_discharge_notes_1['text']\n","y = data_discharge_notes_1['Out_of_hospital_mortality_days']\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T20:52:59.727159Z","iopub.status.busy":"2024-07-28T20:52:59.726894Z","iopub.status.idle":"2024-07-28T20:52:59.758609Z","shell.execute_reply":"2024-07-28T20:52:59.757831Z","shell.execute_reply.started":"2024-07-28T20:52:59.727136Z"},"trusted":true},"outputs":[],"source":["valid_indices = y.dropna().index\n","X = X.loc[valid_indices].reset_index(drop=True)\n","y = y.dropna().reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Basic BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T22:03:29.391510Z","iopub.status.busy":"2024-07-28T22:03:29.390704Z","iopub.status.idle":"2024-07-28T22:05:10.194356Z","shell.execute_reply":"2024-07-28T22:05:10.192913Z","shell.execute_reply.started":"2024-07-28T22:03:29.391477Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Device: CUDA\n","_-_-_-_-_-Tokenizing_-_-_-_-_-_-\n","_-_-_-_-_-Encodings-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Tokenizing:   3%|▎         | 3348/95707 [01:39<45:33, 33.79it/s]  \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m encodings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     28\u001b[0m     encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2989\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2970\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2971\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2986\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2987\u001b[0m     )\n\u001b[1;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3062\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3053\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3054\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3055\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3059\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3060\u001b[0m )\n\u001b[0;32m-> 3062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:157\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    162\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:354\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    352\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_strip_accents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents:\n\u001b[1;32m    356\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_strip_accents(token)\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:370\u001b[0m, in \u001b[0;36mBasicTokenizer._run_strip_accents\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from tqdm import tqdm\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Selected Device: CUDA\")\n","else:\n","    device = torch.device('cpu')\n","    print(\"Selected Device: CPU\")\n","\n","# Load pre-trained BERT tokenizer\n","print(\"_-_-_-_-_-Tokenizing_-_-_-_-_-_-\")\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Example medical text data (X) and continuous target values (y)\n","texts = X.to_list()\n","values = y.to_list()\n","\n","# Tokenize text data with a progress bar\n","print(\"_-_-_-_-_-Encodings-_-_-_-_-_-\")\n","encodings = {\"input_ids\": [], \"attention_mask\": []}\n","\n","for text in tqdm(texts, desc=\"Tokenizing\"):\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    encodings[\"input_ids\"].append(encoding[\"input_ids\"])\n","    encodings[\"attention_mask\"].append(encoding[\"attention_mask\"])\n","\n","# Convert lists to tensors\n","encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n","\n","# Create a dataset class\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, values):\n","        self.encodings = encodings\n","        self.values = values\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.values[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.values)\n","\n","print(\"_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\")\n","dataset = MedicalDataset(encodings, values)\n","\n","# Split into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","print(\"_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\")\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Load pre-trained BERT model with a regression head and move to GPU\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1).to(device)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=8,   \n","    per_device_eval_batch_size=8,    \n","    warmup_steps=500,                \n","    weight_decay=0.01,               \n","    logging_dir='./logs',            \n","    logging_strategy='epoch',  # Use 'epoch' for logging strategy\n","    save_strategy='epoch',     # Ensure save and eval strategies match\n","    evaluation_strategy='epoch', # Set evaluation strategy to 'epoch'\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\"\n",")\n","\n","# Define custom loss function for regression\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.squeeze()\n","    mse = ((preds - labels) ** 2).mean().item()\n","    return {'mse': mse}\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,                        \n","    args=training_args,                 \n","    train_dataset=train_dataset,         \n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","print(\"_-_-_-_-_-Training Started_-_-_-_-_-_-\")\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)\n"]},{"cell_type":"markdown","metadata":{},"source":["# BioBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T22:17:57.609020Z","iopub.status.busy":"2024-07-28T22:17:57.608293Z","iopub.status.idle":"2024-07-28T22:18:42.866727Z","shell.execute_reply":"2024-07-28T22:18:42.865778Z","shell.execute_reply.started":"2024-07-28T22:17:57.608987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Device: CUDA\n","_-_-_-_-_-Tokenizing_-_-_-_-_-_-\n","_-_-_-_-_-Encodings-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Tokenizing: 100%|██████████| 100/100 [00:07<00:00, 12.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\n","_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-Training Started_-_-_-_-_-_-\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 00:34, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>903530.000000</td>\n","      <td>1514880.875000</td>\n","      <td>1514880.875000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>903416.100000</td>\n","      <td>1514652.375000</td>\n","      <td>1514652.625000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>903089.300000</td>\n","      <td>1514253.250000</td>\n","      <td>1514253.250000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1514253.25, 'eval_mse': 1514253.25, 'eval_runtime': 0.299, 'eval_samples_per_second': 66.881, 'eval_steps_per_second': 10.032, 'epoch': 3.0}\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from tqdm import tqdm  # Import tqdm for progress bar\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Selected Device: CUDA\")\n","else:\n","    device = torch.device('cpu')\n","    print(\"Selected Device: CPU\")\n","\n","# Load pre-trained BioBERT tokenizer\n","print(\"_-_-_-_-_-Tokenizing_-_-_-_-_-_-\")\n","tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n","\n","# Example medical text data (X) and continuous target values (y)\n","texts = X.to_list()\n","values = y.to_list()\n","\n","# Tokenize text data with a progress bar\n","print(\"_-_-_-_-_-Encodings-_-_-_-_-_-\")\n","encodings = {'input_ids': [], 'attention_mask': []}\n","\n","# Use tqdm to show progress\n","for text in tqdm(texts, desc=\"Tokenizing\"):\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    encodings['input_ids'].append(encoding['input_ids'])\n","    encodings['attention_mask'].append(encoding['attention_mask'])\n","\n","# Convert lists to tensors\n","encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n","\n","# Create a dataset class\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, values):\n","        self.encodings = encodings\n","        self.values = values\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.values[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.values)\n","\n","print(\"_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\")\n","dataset = MedicalDataset(encodings, values)\n","\n","# Split into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","print(\"_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\")\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Load pre-trained BioBERT model with a regression head and move to GPU\n","model = BertForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', num_labels=1).to(device)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=8,   \n","    per_device_eval_batch_size=8,    \n","    warmup_steps=500,                \n","    weight_decay=0.01,               \n","    logging_dir='./logs',            \n","    logging_strategy='epoch',\n","    eval_strategy='epoch',  # Updated parameter name\n","    logging_steps=10,\n","    save_strategy='epoch',  # Ensure save_strategy matches eval_strategy\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\"\n",")\n","\n","# Define custom loss function for regression\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.squeeze()\n","    mse = ((preds - labels) ** 2).mean().item()\n","    return {'mse': mse}\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,                        \n","    args=training_args,                 \n","    train_dataset=train_dataset,         \n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","print(\"_-_-_-_-_-Training Started_-_-_-_-_-_-\")\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)\n"]},{"cell_type":"markdown","metadata":{},"source":["# PubMedBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T22:24:13.275826Z","iopub.status.busy":"2024-07-28T22:24:13.275108Z","iopub.status.idle":"2024-07-28T22:24:59.023674Z","shell.execute_reply":"2024-07-28T22:24:59.022762Z","shell.execute_reply.started":"2024-07-28T22:24:13.275790Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Device: CUDA\n","_-_-_-_-_-Tokenizing_-_-_-_-_-_-\n","_-_-_-_-_-Encodings-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Tokenizing: 100%|██████████| 100/100 [00:07<00:00, 12.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\n","_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","`evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-Training Started_-_-_-_-_-_-\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 00:34, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1007734.900000</td>\n","      <td>1094219.750000</td>\n","      <td>1094219.750000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1007534.200000</td>\n","      <td>1093850.750000</td>\n","      <td>1093850.750000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1007187.800000</td>\n","      <td>1093295.625000</td>\n","      <td>1093295.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1093295.625, 'eval_mse': 1093295.5, 'eval_runtime': 0.2992, 'eval_samples_per_second': 66.835, 'eval_steps_per_second': 10.025, 'epoch': 3.0}\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from tqdm import tqdm  # Import tqdm for progress bar\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Selected Device: CUDA\")\n","else:\n","    device = torch.device('cpu')\n","    print(\"Selected Device: CPU\")\n","\n","# Load pre-trained PubMedBERT tokenizer\n","print(\"_-_-_-_-_-Tokenizing_-_-_-_-_-_-\")\n","tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n","\n","# Example medical text data (X) and continuous target values (y)\n","texts = X.to_list()\n","values = y.to_list()\n","\n","# Tokenize text data with a progress bar\n","print(\"_-_-_-_-_-Encodings-_-_-_-_-_-\")\n","encodings = {'input_ids': [], 'attention_mask': []}\n","\n","# Use tqdm to show progress\n","for text in tqdm(texts, desc=\"Tokenizing\"):\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    encodings['input_ids'].append(encoding['input_ids'])\n","    encodings['attention_mask'].append(encoding['attention_mask'])\n","\n","# Convert lists to tensors\n","encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n","\n","# Create a dataset class\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, values):\n","        self.encodings = encodings\n","        self.values = values\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.values[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.values)\n","\n","print(\"_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\")\n","dataset = MedicalDataset(encodings, values)\n","\n","# Split into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","print(\"_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\")\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Load pre-trained PubMedBERT model with a regression head and move to GPU\n","model = BertForSequenceClassification.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', num_labels=1).to(device)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=8,   \n","    per_device_eval_batch_size=8,    \n","    warmup_steps=500,                \n","    weight_decay=0.01,               \n","    logging_dir='./logs',            \n","    logging_strategy='epoch',        # Update deprecated argument\n","    evaluation_strategy='epoch',     # Update deprecated argument\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    save_strategy='epoch'            # Ensure save strategy matches eval strategy\n",")\n","\n","# Define custom loss function for regression\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.squeeze()\n","    mse = ((preds - labels) ** 2).mean().item()\n","    return {'mse': mse}\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,                        \n","    args=training_args,                 \n","    train_dataset=train_dataset,         \n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","print(\"_-_-_-_-_-Training Started_-_-_-_-_-_-\")\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Clinical BERT"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T22:26:23.796366Z","iopub.status.busy":"2024-07-28T22:26:23.795985Z","iopub.status.idle":"2024-07-28T22:26:59.650301Z","shell.execute_reply":"2024-07-28T22:26:59.649282Z","shell.execute_reply.started":"2024-07-28T22:26:23.796335Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Device: CUDA\n","_-_-_-_-_-Tokenizing_-_-_-_-_-_-\n","_-_-_-_-_-Encodings-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","Tokenizing: 100%|██████████| 95707/95707 [14:45<00:00, 108.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\n","_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"ImportError","evalue":"Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Ensure save strategy matches eval strategy\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Define custom loss function for regression\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(pred):\n","File \u001b[0;32m<string>:128\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics)\u001b[0m\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/training_args.py:1641\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1641\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_3)\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1665\u001b[0m ):\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/training_args.py:2149\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/utils/generic.py:59\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n","File \u001b[0;32m~/.conda/envs/jupyter-notebook/lib/python3.10/site-packages/transformers/training_args.py:2055\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2055\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2056\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2058\u001b[0m         )\n\u001b[1;32m   2059\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"]}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from tqdm import tqdm  # Import tqdm for progress bar\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Selected Device: CUDA\")\n","else:\n","    device = torch.device('cpu')\n","    print(\"Selected Device: CPU\")\n","\n","# Load pre-trained ClinicalBERT tokenizer\n","print(\"_-_-_-_-_-Tokenizing_-_-_-_-_-_-\")\n","tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","\n","# Example medical text data (X) and continuous target values (y)\n","texts = X.to_list()\n","values = y.to_list()\n","\n","# Tokenize text data with a progress bar\n","print(\"_-_-_-_-_-Encodings-_-_-_-_-_-\")\n","encodings = {'input_ids': [], 'attention_mask': []}\n","\n","# Use tqdm to show progress\n","for text in tqdm(texts, desc=\"Tokenizing\"):\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    encodings['input_ids'].append(encoding['input_ids'])\n","    encodings['attention_mask'].append(encoding['attention_mask'])\n","\n","# Convert lists to tensors\n","encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n","\n","# Create a dataset class\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, values):\n","        self.encodings = encodings\n","        self.values = values\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.values[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.values)\n","\n","print(\"_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\")\n","dataset = MedicalDataset(encodings, values)\n","\n","# Split into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","print(\"_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\")\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Load pre-trained ClinicalBERT model with a regression head and move to GPU\n","model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', num_labels=1).to(device)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=8,   \n","    per_device_eval_batch_size=8,    \n","    warmup_steps=500,                \n","    weight_decay=0.01,               \n","    logging_dir='./logs',            \n","    logging_strategy='epoch',\n","    evaluation_strategy='epoch',\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    save_strategy='epoch'            # Ensure save strategy matches eval strategy\n",")\n","\n","# Define custom loss function for regression\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.squeeze()\n","    mse = ((preds - labels) ** 2).mean().item()\n","    return {'mse': mse}\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,                        \n","    args=training_args,                 \n","    train_dataset=train_dataset,         \n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","print(\"_-_-_-_-_-Training Started_-_-_-_-_-_-\")\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)\n"]},{"cell_type":"markdown","metadata":{},"source":["# ClinicalXLNet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-28T22:29:13.024675Z","iopub.status.busy":"2024-07-28T22:29:13.023997Z","iopub.status.idle":"2024-07-28T22:30:12.324160Z","shell.execute_reply":"2024-07-28T22:30:12.322987Z","shell.execute_reply.started":"2024-07-28T22:29:13.024642Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Device: CUDA\n","_-_-_-_-_-Tokenizing_-_-_-_-_-_-\n","_-_-_-_-_-Encodings-_-_-_-_-_-\n"]},{"name":"stderr","output_type":"stream","text":["Tokenizing: 100%|██████████| 100/100 [00:02<00:00, 43.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\n","_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dbe035bc09649cb99596646b236d192","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","`evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n"]},{"name":"stdout","output_type":"stream","text":["_-_-_-_-_-Training Started_-_-_-_-_-_-\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 00:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>996446.100000</td>\n","      <td>1140349.750000</td>\n","      <td>1140349.750000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>996105.900000</td>\n","      <td>1139790.750000</td>\n","      <td>1139790.875000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>995228.900000</td>\n","      <td>1138596.375000</td>\n","      <td>1138596.500000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1138596.375, 'eval_mse': 1138596.5, 'eval_runtime': 0.7821, 'eval_samples_per_second': 25.571, 'eval_steps_per_second': 3.836, 'epoch': 3.0}\n"]}],"source":["from transformers import XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from tqdm import tqdm\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(\"Selected Device: CUDA\")\n","else:\n","    device = torch.device('cpu')\n","    print(\"Selected Device: CPU\")\n","\n","# Load pre-trained XLNet tokenizer\n","print(\"_-_-_-_-_-Tokenizing_-_-_-_-_-_-\")\n","tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')  # Change to a valid model\n","\n","# Example medical text data (X) and continuous target values (y)\n","texts = X.to_list()\n","values = y.to_list()\n","\n","# Tokenize text data with a progress bar\n","print(\"_-_-_-_-_-Encodings-_-_-_-_-_-\")\n","encodings = {'input_ids': [], 'attention_mask': []}\n","\n","# Use tqdm to show progress\n","for text in tqdm(texts, desc=\"Tokenizing\"):\n","    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    encodings['input_ids'].append(encoding['input_ids'])\n","    encodings['attention_mask'].append(encoding['attention_mask'])\n","\n","# Convert lists to tensors\n","encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n","\n","# Create a dataset class\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, values):\n","        self.encodings = encodings\n","        self.values = values\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.values[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.values)\n","\n","print(\"_-_-_-_-_-MedicalDataset Creation-_-_-_-_-_-\")\n","dataset = MedicalDataset(encodings, values)\n","\n","# Split into train and test sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","print(\"_-_-_-_-_-Splitting into train and test sets-_-_-_-_-_-\")\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Load pre-trained XLNet model with a regression head and move to GPU\n","model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=1).to(device)  # Change to a valid model\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=8,   \n","    per_device_eval_batch_size=8,    \n","    warmup_steps=500,                \n","    weight_decay=0.01,               \n","    logging_dir='./logs',            \n","    logging_strategy='epoch',\n","    evaluation_strategy='epoch',\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    save_strategy='epoch'            \n",")\n","\n","# Define custom loss function for regression\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.squeeze()\n","    mse = ((preds - labels) ** 2).mean().item()\n","    return {'mse': mse}\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,                        \n","    args=training_args,                 \n","    train_dataset=train_dataset,         \n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","print(\"_-_-_-_-_-Training Started_-_-_-_-_-_-\")\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5301896,"sourceId":9041465,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
