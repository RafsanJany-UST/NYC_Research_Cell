{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd7eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15116\\4173027433.py:2: DtypeWarning: Columns (108,112,113,117,151,154,161,162,164,169,180,185,198,221,228,264,267,269,270,306,307,310,311,314,315,318,319,322,323,326,328,330,331,334,335,338,339,342,343,346,348,350,352,354,356,358,362,363,393,403,404,406,418,423,430,433,436,439,444) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "path =  r\"D:\\Data\\NYC\\KINZ\\KINECT_ACC_dataset_with_qor15_2025-05-27_14-29PM.csv\"\n",
    "dataframe = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c59401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientID</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>walking_speed</th>\n",
       "      <th>t_uniform</th>\n",
       "      <th>PELVIS_X</th>\n",
       "      <th>SPINE_NAVAL_X</th>\n",
       "      <th>SPINE_CHEST_X</th>\n",
       "      <th>NECK_X</th>\n",
       "      <th>CLAVICLE_LEFT_X</th>\n",
       "      <th>SHOULDER_LEFT_X</th>\n",
       "      <th>...</th>\n",
       "      <th>comp4_type</th>\n",
       "      <th>comp4_cd</th>\n",
       "      <th>comp4_desc</th>\n",
       "      <th>comp5_type</th>\n",
       "      <th>comp5_cd</th>\n",
       "      <th>comp5_desc</th>\n",
       "      <th>cc_index</th>\n",
       "      <th>notes</th>\n",
       "      <th>postoperative_3day_data_complete</th>\n",
       "      <th>QoR_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001-LO</td>\n",
       "      <td>Trial4</td>\n",
       "      <td>Fast</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>-36.173563</td>\n",
       "      <td>-32.222103</td>\n",
       "      <td>-29.139376</td>\n",
       "      <td>-24.572845</td>\n",
       "      <td>6.011915</td>\n",
       "      <td>125.887829</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001-LO</td>\n",
       "      <td>Trial4</td>\n",
       "      <td>Fast</td>\n",
       "      <td>0.021627</td>\n",
       "      <td>-26.631417</td>\n",
       "      <td>-24.712284</td>\n",
       "      <td>-23.071704</td>\n",
       "      <td>-19.095052</td>\n",
       "      <td>11.770258</td>\n",
       "      <td>132.878366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001-LO</td>\n",
       "      <td>Trial4</td>\n",
       "      <td>Fast</td>\n",
       "      <td>0.041607</td>\n",
       "      <td>-17.089270</td>\n",
       "      <td>-17.202465</td>\n",
       "      <td>-17.004033</td>\n",
       "      <td>-13.617259</td>\n",
       "      <td>17.528602</td>\n",
       "      <td>139.868903</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001-LO</td>\n",
       "      <td>Trial4</td>\n",
       "      <td>Fast</td>\n",
       "      <td>0.061588</td>\n",
       "      <td>-7.547124</td>\n",
       "      <td>-9.692646</td>\n",
       "      <td>-10.936362</td>\n",
       "      <td>-8.139466</td>\n",
       "      <td>23.286945</td>\n",
       "      <td>146.859440</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001-LO</td>\n",
       "      <td>Trial4</td>\n",
       "      <td>Fast</td>\n",
       "      <td>0.081568</td>\n",
       "      <td>-13.797027</td>\n",
       "      <td>-19.793992</td>\n",
       "      <td>-23.898339</td>\n",
       "      <td>-14.030918</td>\n",
       "      <td>16.551726</td>\n",
       "      <td>142.158210</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116578</th>\n",
       "      <td>104-AB</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>4.830416</td>\n",
       "      <td>-106.625566</td>\n",
       "      <td>-99.957650</td>\n",
       "      <td>-94.666152</td>\n",
       "      <td>-85.579490</td>\n",
       "      <td>-57.789959</td>\n",
       "      <td>56.025031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116579</th>\n",
       "      <td>104-AB</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>4.850396</td>\n",
       "      <td>-102.713773</td>\n",
       "      <td>-95.755603</td>\n",
       "      <td>-90.207959</td>\n",
       "      <td>-81.320193</td>\n",
       "      <td>-53.332484</td>\n",
       "      <td>61.004254</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116580</th>\n",
       "      <td>104-AB</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>4.870377</td>\n",
       "      <td>-99.019364</td>\n",
       "      <td>-91.877139</td>\n",
       "      <td>-86.144402</td>\n",
       "      <td>-77.671170</td>\n",
       "      <td>-49.452012</td>\n",
       "      <td>65.611329</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116581</th>\n",
       "      <td>104-AB</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>4.890357</td>\n",
       "      <td>-95.402811</td>\n",
       "      <td>-88.318498</td>\n",
       "      <td>-82.601346</td>\n",
       "      <td>-74.990126</td>\n",
       "      <td>-46.483008</td>\n",
       "      <td>69.709487</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116582</th>\n",
       "      <td>104-AB</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>4.910338</td>\n",
       "      <td>-91.786259</td>\n",
       "      <td>-84.759857</td>\n",
       "      <td>-79.058291</td>\n",
       "      <td>-72.309082</td>\n",
       "      <td>-43.514004</td>\n",
       "      <td>73.807645</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116583 rows Ã— 447 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patientID trial_num walking_speed  t_uniform    PELVIS_X  \\\n",
       "0         001-LO    Trial4          Fast   0.001646  -36.173563   \n",
       "1         001-LO    Trial4          Fast   0.021627  -26.631417   \n",
       "2         001-LO    Trial4          Fast   0.041607  -17.089270   \n",
       "3         001-LO    Trial4          Fast   0.061588   -7.547124   \n",
       "4         001-LO    Trial4          Fast   0.081568  -13.797027   \n",
       "...          ...       ...           ...        ...         ...   \n",
       "116578    104-AB    Trial2       Regular   4.830416 -106.625566   \n",
       "116579    104-AB    Trial2       Regular   4.850396 -102.713773   \n",
       "116580    104-AB    Trial2       Regular   4.870377  -99.019364   \n",
       "116581    104-AB    Trial2       Regular   4.890357  -95.402811   \n",
       "116582    104-AB    Trial2       Regular   4.910338  -91.786259   \n",
       "\n",
       "        SPINE_NAVAL_X  SPINE_CHEST_X     NECK_X  CLAVICLE_LEFT_X  \\\n",
       "0          -32.222103     -29.139376 -24.572845         6.011915   \n",
       "1          -24.712284     -23.071704 -19.095052        11.770258   \n",
       "2          -17.202465     -17.004033 -13.617259        17.528602   \n",
       "3           -9.692646     -10.936362  -8.139466        23.286945   \n",
       "4          -19.793992     -23.898339 -14.030918        16.551726   \n",
       "...               ...            ...        ...              ...   \n",
       "116578     -99.957650     -94.666152 -85.579490       -57.789959   \n",
       "116579     -95.755603     -90.207959 -81.320193       -53.332484   \n",
       "116580     -91.877139     -86.144402 -77.671170       -49.452012   \n",
       "116581     -88.318498     -82.601346 -74.990126       -46.483008   \n",
       "116582     -84.759857     -79.058291 -72.309082       -43.514004   \n",
       "\n",
       "        SHOULDER_LEFT_X  ...  comp4_type  comp4_cd  comp4_desc  comp5_type  \\\n",
       "0            125.887829  ...         NaN       NaN         NaN         NaN   \n",
       "1            132.878366  ...         NaN       NaN         NaN         NaN   \n",
       "2            139.868903  ...         NaN       NaN         NaN         NaN   \n",
       "3            146.859440  ...         NaN       NaN         NaN         NaN   \n",
       "4            142.158210  ...         NaN       NaN         NaN         NaN   \n",
       "...                 ...  ...         ...       ...         ...         ...   \n",
       "116578        56.025031  ...         NaN       NaN         NaN         NaN   \n",
       "116579        61.004254  ...         NaN       NaN         NaN         NaN   \n",
       "116580        65.611329  ...         NaN       NaN         NaN         NaN   \n",
       "116581        69.709487  ...         NaN       NaN         NaN         NaN   \n",
       "116582        73.807645  ...         NaN       NaN         NaN         NaN   \n",
       "\n",
       "        comp5_cd  comp5_desc  cc_index  notes  \\\n",
       "0            NaN         NaN       0.0    NaN   \n",
       "1            NaN         NaN       0.0    NaN   \n",
       "2            NaN         NaN       0.0    NaN   \n",
       "3            NaN         NaN       0.0    NaN   \n",
       "4            NaN         NaN       0.0    NaN   \n",
       "...          ...         ...       ...    ...   \n",
       "116578       NaN         NaN       NaN    NaN   \n",
       "116579       NaN         NaN       NaN    NaN   \n",
       "116580       NaN         NaN       NaN    NaN   \n",
       "116581       NaN         NaN       NaN    NaN   \n",
       "116582       NaN         NaN       NaN    NaN   \n",
       "\n",
       "        postoperative_3day_data_complete  QoR_class  \n",
       "0                                    2.0        1.0  \n",
       "1                                    2.0        1.0  \n",
       "2                                    2.0        1.0  \n",
       "3                                    2.0        1.0  \n",
       "4                                    2.0        1.0  \n",
       "...                                  ...        ...  \n",
       "116578                               0.0        NaN  \n",
       "116579                               0.0        NaN  \n",
       "116580                               0.0        NaN  \n",
       "116581                               0.0        NaN  \n",
       "116582                               0.0        NaN  \n",
       "\n",
       "[116583 rows x 447 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e464a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15116\\5513061.py:18: DtypeWarning: Columns (108,112,113,117,151,154,161,162,164,169,180,185,198,221,228,264,267,269,270,306,307,310,311,314,315,318,319,322,323,326,328,330,331,334,335,338,339,342,343,346,348,350,352,354,356,358,362,363,393,403,404,406,418,423,430,433,436,439,444) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 0.5386, Acc: 0.5900 | Test Loss: 0.5647, Acc: 0.6210\n",
      "Epoch 002 | Train Loss: 0.5145, Acc: 0.6129 | Test Loss: 0.5466, Acc: 0.6375\n",
      "Epoch 003 | Train Loss: 0.4938, Acc: 0.6185 | Test Loss: 0.5235, Acc: 0.6838\n",
      "Epoch 004 | Train Loss: 0.4741, Acc: 0.6310 | Test Loss: 0.5155, Acc: 0.6908\n",
      "Epoch 005 | Train Loss: 0.4539, Acc: 0.6546 | Test Loss: 0.4933, Acc: 0.7142\n",
      "Epoch 006 | Train Loss: 0.4326, Acc: 0.6854 | Test Loss: 0.4609, Acc: 0.7302\n",
      "Epoch 007 | Train Loss: 0.4109, Acc: 0.7078 | Test Loss: 0.4478, Acc: 0.7504\n",
      "Epoch 008 | Train Loss: 0.3904, Acc: 0.7293 | Test Loss: 0.4205, Acc: 0.7773\n",
      "Epoch 009 | Train Loss: 0.3683, Acc: 0.7506 | Test Loss: 0.3935, Acc: 0.7970\n",
      "Epoch 010 | Train Loss: 0.3442, Acc: 0.7746 | Test Loss: 0.3651, Acc: 0.8126\n",
      "Epoch 011 | Train Loss: 0.3222, Acc: 0.7923 | Test Loss: 0.3414, Acc: 0.8370\n",
      "Epoch 012 | Train Loss: 0.3033, Acc: 0.8085 | Test Loss: 0.3338, Acc: 0.8335\n",
      "Epoch 013 | Train Loss: 0.2868, Acc: 0.8207 | Test Loss: 0.3240, Acc: 0.8313\n",
      "Epoch 014 | Train Loss: 0.2692, Acc: 0.8369 | Test Loss: 0.2949, Acc: 0.8594\n",
      "Epoch 015 | Train Loss: 0.2559, Acc: 0.8410 | Test Loss: 0.2755, Acc: 0.8612\n",
      "Epoch 016 | Train Loss: 0.2362, Acc: 0.8573 | Test Loss: 0.2572, Acc: 0.8691\n",
      "Epoch 017 | Train Loss: 0.2251, Acc: 0.8677 | Test Loss: 0.2349, Acc: 0.9044\n",
      "Epoch 018 | Train Loss: 0.2124, Acc: 0.8750 | Test Loss: 0.2183, Acc: 0.9022\n",
      "Epoch 019 | Train Loss: 0.1958, Acc: 0.8895 | Test Loss: 0.2090, Acc: 0.9134\n",
      "Epoch 020 | Train Loss: 0.1877, Acc: 0.8910 | Test Loss: 0.2154, Acc: 0.9042\n",
      "Epoch 021 | Train Loss: 0.1780, Acc: 0.8997 | Test Loss: 0.1777, Acc: 0.9292\n",
      "Epoch 022 | Train Loss: 0.1643, Acc: 0.9088 | Test Loss: 0.2043, Acc: 0.9068\n",
      "Epoch 023 | Train Loss: 0.1527, Acc: 0.9152 | Test Loss: 0.2319, Acc: 0.8892\n",
      "Epoch 024 | Train Loss: 0.1468, Acc: 0.9201 | Test Loss: 0.1673, Acc: 0.9221\n",
      "Epoch 025 | Train Loss: 0.1373, Acc: 0.9270 | Test Loss: 0.1421, Acc: 0.9462\n",
      "Epoch 026 | Train Loss: 0.1289, Acc: 0.9332 | Test Loss: 0.1443, Acc: 0.9419\n",
      "Epoch 027 | Train Loss: 0.1216, Acc: 0.9359 | Test Loss: 0.1279, Acc: 0.9493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    145\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 95\u001b[0m, in \u001b[0;36mHybridSTGCN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     93\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn1(x, edge_index))\n\u001b[1;32m---> 95\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     96\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:266\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "path =  r\"D:\\Data\\NYC\\KINZ\\KINECT_ACC_dataset_with_qor15_2025-05-27_14-29PM.csv\"\n",
    "dataframe = pd.read_csv(path)\n",
    "dataframe = dataframe[dataframe['walking_speed'] == 'Fast']\n",
    "\n",
    "\n",
    "# 2. Graph Definition\n",
    "joints = ['PELVIS', 'SPINE_NAVAL', 'SPINE_CHEST', 'NECK', 'CLAVICLE_LEFT', 'SHOULDER_LEFT',\n",
    "          'ELBOW_LEFT', 'WRIST_LEFT', 'HAND_LEFT', 'HANDTIP_LEFT', 'THUMB_LEFT',\n",
    "          'CLAVICLE_RIGHT', 'SHOULDER_RIGHT', 'ELBOW_RIGHT', 'WRIST_RIGHT', 'HAND_RIGHT',\n",
    "          'HANDTIP_RIGHT', 'THUMB_RIGHT', 'HIP_LEFT', 'KNEE_LEFT', 'ANKLE_LEFT',\n",
    "          'FOOT_LEFT', 'HIP_RIGHT', 'KNEE_RIGHT', 'ANKLE_RIGHT', 'FOOT_RIGHT',\n",
    "          'HEAD', 'NOSE', 'EYE_LEFT', 'EAR_LEFT', 'EYE_RIGHT', 'EAR_RIGHT']\n",
    "\n",
    "edges = [('PELVIS', 'SPINE_NAVAL'), ('SPINE_NAVAL', 'SPINE_CHEST'), ('SPINE_CHEST', 'NECK'),\n",
    "         ('NECK', 'HEAD'), ('SPINE_CHEST', 'CLAVICLE_LEFT'), ('CLAVICLE_LEFT', 'SHOULDER_LEFT'),\n",
    "         ('SHOULDER_LEFT', 'ELBOW_LEFT'), ('ELBOW_LEFT', 'WRIST_LEFT'), ('WRIST_LEFT', 'HAND_LEFT'),\n",
    "         ('HAND_LEFT', 'HANDTIP_LEFT'), ('WRIST_LEFT', 'THUMB_LEFT'), ('SPINE_CHEST', 'CLAVICLE_RIGHT'),\n",
    "         ('CLAVICLE_RIGHT', 'SHOULDER_RIGHT'), ('SHOULDER_RIGHT', 'ELBOW_RIGHT'), ('ELBOW_RIGHT', 'WRIST_RIGHT'),\n",
    "         ('WRIST_RIGHT', 'HAND_RIGHT'), ('HAND_RIGHT', 'HANDTIP_RIGHT'), ('WRIST_RIGHT', 'THUMB_RIGHT'),\n",
    "         ('PELVIS', 'HIP_LEFT'), ('HIP_LEFT', 'KNEE_LEFT'), ('KNEE_LEFT', 'ANKLE_LEFT'),\n",
    "         ('ANKLE_LEFT', 'FOOT_LEFT'), ('PELVIS', 'HIP_RIGHT'), ('HIP_RIGHT', 'KNEE_RIGHT'),\n",
    "         ('KNEE_RIGHT', 'ANKLE_RIGHT'), ('ANKLE_RIGHT', 'FOOT_RIGHT'),\n",
    "         ('HEAD', 'NOSE'), ('HEAD','EYE_LEFT'), ('HEAD', 'EYE_RIGHT'), ('HEAD', 'EAR_LEFT'), ('HEAD', 'EAR_RIGHT')]\n",
    "\n",
    "joint_to_idx = {joint: idx for idx, joint in enumerate(joints)}\n",
    "edge_index = torch.tensor([[joint_to_idx[src], joint_to_idx[dst]] for src, dst in edges] +\n",
    "                          [[joint_to_idx[dst], joint_to_idx[src]] for src, dst in edges],\n",
    "                          dtype=torch.long).t()\n",
    "\n",
    "# 3. Dataset\n",
    "class SkeletonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        self.num_nodes = len(joints)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        node_features = [[row[f'{j}_X'], row[f'{j}_Y'], row[f'{j}_Z'], row['t_uniform']] for j in joints]\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        y = torch.tensor(row['QoR_class'], dtype=torch.float)\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# 4. Random split\n",
    "full_dataset = SkeletonDataset(dataframe.dropna(subset=['QoR_class']))\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# 5. Weighted Sampling\n",
    "train_indices = train_dataset.indices if isinstance(train_dataset, torch.utils.data.Subset) else list(range(len(train_dataset)))\n",
    "train_df = dataframe.iloc[train_indices].reset_index(drop=True)\n",
    "train_df = train_df.dropna(subset=['QoR_class'])\n",
    "class_counts = train_df['QoR_class'].value_counts().sort_index().to_numpy()\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[int(label)] for label in train_df['QoR_class']]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# 6. Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 7. Model\n",
    "class HybridSTGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.temporal_conv1 = nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_channels, hidden_channels, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        batch = getattr(data, 'batch', None)\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "        batch_size = batch.max().item() + 1 if batch is not None else 1\n",
    "        num_nodes = x.size(0) // batch_size\n",
    "        x = x.view(batch_size, num_nodes, -1).permute(0, 2, 1)\n",
    "        x = F.relu(self.temporal_conv1(x)).permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "# 8. Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridSTGCN(in_channels=4, hidden_channels=128, num_classes=1).to(device)\n",
    "pos_weight = torch.tensor([class_weights[0] / class_weights[1]]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.005):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.best_model is not None:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# 9. Training Loop\n",
    "num_epochs = 100\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        y = data.y.to(device).float()\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        correct += ((out >= 0.5) == y).sum().item()\n",
    "        total += data.num_graphs\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, correct, total, all_preds, all_labels = 0, 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            y = data.y.to(device).float()\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            pred = (torch.sigmoid(out) >= 0.5).float()\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += data.num_graphs\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss = total_loss / len(test_loader.dataset)\n",
    "    test_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")\n",
    "\n",
    "    if early_stopping(test_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# 10. Final Evaluation\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Good (0)', 'Not Good (1)']))\n",
    "\n",
    "# 11. Save Model + Stats\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(model.state_dict(), f\"QOR_hybrid_stgcn_model_{now}.pth\")\n",
    "with open(f\"QOR_hybrid_stgcn_model_statistics_{now}.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'final_metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'specificity': specificity,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "    }, f)\n",
    "print(\"Model and statistics saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634d5cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timeStamps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timeStamps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m X \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQoR_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatientID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial_num\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalking_speed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Concatenate 'timeStamps' as an additional column to the X dataset\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X, \u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeStamps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQoR_class\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m groups \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatientID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timeStamps'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "#dataframe = pd.read_csv(r'C:\\Users\\RAZER\\Desktop\\KINZ AGAIN\\Kinect_feature_matrix_20250307_123820.csv')\n",
    "\n",
    "# Prepare data\n",
    "# Include 'timeStamps' as an additional feature\n",
    "X = dataframe.drop(columns=['QoR_class', 'patientID', 'trial_num', 'walking_speed'])\n",
    "\n",
    "# Concatenate 'timeStamps' as an additional column to the X dataset\n",
    "X = pd.concat([X, dataframe['timeStamps']], axis=1)\n",
    "\n",
    "y = dataframe['QoR_class']\n",
    "groups = dataframe['patientID']\n",
    "\n",
    "# Normalize data (standardization)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test using GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# CNN-LSTM Hybrid Model Definition\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.5):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "        # Initialize hidden_size as an attribute of the model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN Layers for feature extraction\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(256, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 for bidirectional\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply CNN layers with Batch Normalization\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Prepare data for LSTM (must reshape to [batch_size, seq_len, input_size])\n",
    "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        h0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)  # 2 for bidirectional\n",
    "        c0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size = X_train.shape[1]  # Now includes 'timeStamps', so input_size should be 102\n",
    "hidden_size = 128  # Increased hidden size to capture more complex patterns\n",
    "output_size = len(y.unique())  # Number of classes\n",
    "model = CNNLSTMModel(input_size=1, hidden_size=hidden_size, output_size=output_size, num_layers=3, dropout=0.5)  # Adjusted input_size for CNN\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001)  # Lower learning rate and higher weight decay for L2 regularization\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 5\n",
    "best_test_acc = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reshape the data to [batch_size, num_channels, sequence_length]\n",
    "        X_batch = X_batch.unsqueeze(1)  # Adding channel dimension (1 for single feature per time step)\n",
    "        \n",
    "        outputs = model(X_batch)  # Pass through CNN-LSTM model\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test, total_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Reshape the data to [batch_size, num_channels, sequence_length]\n",
    "            X_batch = X_batch.unsqueeze(1)  # Adding channel dimension (1 for single feature per time step)\n",
    "            \n",
    "            outputs = model(X_batch)  # Pass through CNN-LSTM model\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            correct_test += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model.state_dict(), 'QOR_classification_cnn_lstm.pth')\n",
    "print(\"CNN-LSTM Model saved successfully!\")\n",
    "\n",
    "# Final Test Accuracy\n",
    "print(f'Final Test Accuracy: {test_accuracies[-1]:.2f}%')\n",
    "\n",
    "# Confusion Matrix\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Reshape the data to [batch_size, num_channels, sequence_length]\n",
    "        X_batch = X_batch.unsqueeze(1)  # Adding channel dimension (1 for single feature per time step)\n",
    "        \n",
    "        outputs = model(X_batch)  # Pass through CNN-LSTM model\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_true_labels.extend(y_batch.cpu().numpy())\n",
    "        all_pred_labels.extend(predicted.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102c12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe4138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
