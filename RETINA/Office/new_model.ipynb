{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd029ca3",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb17c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to dataset folders\n",
    "ddiabetic_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\"\n",
    "normal_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\\Normal\"\n",
    "\n",
    "# Transformations for training data (with augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Transformations for validation and test data (without augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Combine both folders into a single dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=r\"D:\\Data\\NYC\\Retina\\Data\"\n",
    ")\n",
    "\n",
    "# Explicitly verify class names and indices\n",
    "print(f\"Class to index mapping: {dataset.class_to_idx}\")\n",
    "\n",
    "# Splitting indices for train, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    list(range(len(dataset))),\n",
    "    test_size=0.3,  # 30% for validation and test\n",
    "    stratify=dataset.targets,  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,  # Split remaining 30% equally into validation and test\n",
    "    stratify=[dataset.targets[i] for i in temp_idx],  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create subsets for DataLoader\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Apply transformations to subsets\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Verify dataset sizes\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "print(f\"Test images: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "GLOBAL_EPOCH = 250          # or whatever value you want\n",
    "GLOBAL_Lr = 5e-4           # or any learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eva_type = \"full\"          # (example) naming the evaluation file\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # For logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr=0.001, wd=1e-4,\n",
    "                log_dir=\"./logs\", save_path=\"best_model.pth\", patience=5,\n",
    "                use_scheduler=True, use_mixed_precision=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    scaler = amp.GradScaler() if use_mixed_precision else None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with amp.autocast(enabled=use_mixed_precision):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            if use_mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "\n",
    "        # Save best model weights\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Validation loss improved. Model weights saved!\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement in validation loss for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs! Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "    # Plotting\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # <-- For saving CSV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tqdm.auto import tqdm  # <-- Smoother tqdm for all environments (optional but better)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, model_name,\n",
    "                   checkpoint_path=None, output_file=\"evaluation_results.txt\",\n",
    "                   save_dir=\"./results\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # If checkpoint is provided, load it\n",
    "    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}...\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "            if labels.ndim == 1:\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Handle output if model outputs a dict (like ViT sometimes)\n",
    "            if isinstance(outputs, dict) and \"logits\" in outputs:\n",
    "                outputs = outputs[\"logits\"]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    # Explicit class names + class order handling\n",
    "    class_names = ['Diabetic', 'Normal']\n",
    "    class_labels = [0, 1]\n",
    "\n",
    "    # Ensure labels and preds are flattened properly\n",
    "    all_labels = [int(x[0]) for x in all_labels]  # Flatten nested arrays\n",
    "    all_preds = [int(x[0]) for x in all_preds]\n",
    "\n",
    "    # Classification report (handle missing classes safely)\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Confusion matrix (explicit label order)\n",
    "    cm = confusion_matrix(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels\n",
    "    )\n",
    "\n",
    "    # Save text results\n",
    "    output_path = os.path.join(save_dir, output_file)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        results = (\n",
    "            f\"Model: {model_name}\\n\"\n",
    "            f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "            f\"Test Accuracy: {test_accuracy:.4f}\\n\"\n",
    "            f\"Classification Report:\\n{report}\\n\"\n",
    "            f\"Confusion Matrix:\\n{cm}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        print(results)\n",
    "        f.write(results)\n",
    "\n",
    "    # -------- Confusion Matrix Plot --------\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # -------- ROC Curve --------\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"{model_name} ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    roc_save_path = os.path.join(save_dir, f\"{model_name}_roc_curve.png\")\n",
    "    plt.savefig(roc_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _____________________________Training Part______________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "import gc\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# ------------------------ Setup Model ------------------------\n",
    "efficientnet = efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "num_features = efficientnet.classifier[1].in_features\n",
    "efficientnet.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1)  # Output single logit (No Sigmoid because we'll use BCEWithLogitsLoss)\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------ Training ------------------------\n",
    "save_path = \"efficientnet_b0_best_model.pth\"\n",
    "log_dir = \"./logs/efficientnet\"\n",
    "\n",
    "train_model(\n",
    "    model=efficientnet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=GLOBAL_EPOCH,\n",
    "    lr=GLOBAL_Lr,\n",
    "    wd=1e-4,\n",
    "    log_dir=log_dir,\n",
    "    save_path=save_path,\n",
    "    patience=5,                 # Early stopping patience\n",
    "    use_scheduler=True,         # Enable ReduceLROnPlateau\n",
    "    use_mixed_precision=True    # AMP for faster training\n",
    ")\n",
    "\n",
    "# ------------------------ Load the Best Model ------------------------\n",
    "efficientnet.load_state_dict(torch.load(save_path))  # Load the best checkpoint before testing\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "evaluate_model(\n",
    "    model=efficientnet,\n",
    "    test_loader=test_loader,\n",
    "    criterion=nn.BCEWithLogitsLoss(),  # Important: BCEWithLogitsLoss\n",
    "    model_name=\"efficientnet_b0\",\n",
    "    output_file=\"evaluation_results.txt\",  # File to save evaluation text results\n",
    "    save_dir=\"./results/efficientnet\"      # Folder where ROC, CM, and text results will be saved\n",
    ")\n",
    "\n",
    "# ------------------------ Cleanup ------------------------\n",
    "import gc\n",
    "del efficientnet\n",
    "torch.cuda.empty_cache()  # Clear the GPU memory\n",
    "gc.collect()              # Garbage collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b4da2",
   "metadata": {},
   "source": [
    "# efficientnet_cbam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a015a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to dataset folders\n",
    "ddiabetic_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\"\n",
    "normal_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\\Normal\"\n",
    "\n",
    "# Transformations for training data (with augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Transformations for validation and test data (without augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Combine both folders into a single dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=r\"D:\\Data\\NYC\\Retina\\Data\"\n",
    ")\n",
    "\n",
    "# Explicitly verify class names and indices\n",
    "print(f\"Class to index mapping: {dataset.class_to_idx}\")\n",
    "\n",
    "# Splitting indices for train, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    list(range(len(dataset))),\n",
    "    test_size=0.3,  # 30% for validation and test\n",
    "    stratify=dataset.targets,  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,  # Split remaining 30% equally into validation and test\n",
    "    stratify=[dataset.targets[i] for i in temp_idx],  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create subsets for DataLoader\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Apply transformations to subsets\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Verify dataset sizes\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "print(f\"Test images: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "GLOBAL_EPOCH = 250          # or whatever value you want\n",
    "GLOBAL_Lr = 5e-4           # or any learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eva_type = \"full\"          # (example) naming the evaluation file\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # For logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr=0.001, wd=1e-4,\n",
    "                log_dir=\"./logs\", save_path=\"best_model.pth\", patience=5,\n",
    "                use_scheduler=True, use_mixed_precision=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    scaler = amp.GradScaler() if use_mixed_precision else None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with amp.autocast(enabled=use_mixed_precision):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            if use_mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "\n",
    "        # Save best model weights\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Validation loss improved. Model weights saved!\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement in validation loss for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs! Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "    # Plotting\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # <-- For saving CSV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tqdm.auto import tqdm  # <-- Smoother tqdm for all environments (optional but better)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, model_name,\n",
    "                   checkpoint_path=None, output_file=\"evaluation_results.txt\",\n",
    "                   save_dir=\"./results\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # If checkpoint is provided, load it\n",
    "    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}...\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "            if labels.ndim == 1:\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Handle output if model outputs a dict (like ViT sometimes)\n",
    "            if isinstance(outputs, dict) and \"logits\" in outputs:\n",
    "                outputs = outputs[\"logits\"]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    # Explicit class names + class order handling\n",
    "    class_names = ['Diabetic', 'Normal']\n",
    "    class_labels = [0, 1]\n",
    "\n",
    "    # Ensure labels and preds are flattened properly\n",
    "    all_labels = [int(x[0]) for x in all_labels]  # Flatten nested arrays\n",
    "    all_preds = [int(x[0]) for x in all_preds]\n",
    "\n",
    "    # Classification report (handle missing classes safely)\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Confusion matrix (explicit label order)\n",
    "    cm = confusion_matrix(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels\n",
    "    )\n",
    "\n",
    "    # Save text results\n",
    "    output_path = os.path.join(save_dir, output_file)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        results = (\n",
    "            f\"Model: {model_name}\\n\"\n",
    "            f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "            f\"Test Accuracy: {test_accuracy:.4f}\\n\"\n",
    "            f\"Classification Report:\\n{report}\\n\"\n",
    "            f\"Confusion Matrix:\\n{cm}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        print(results)\n",
    "        f.write(results)\n",
    "\n",
    "    # -------- Confusion Matrix Plot --------\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # -------- ROC Curve --------\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"{model_name} ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    roc_save_path = os.path.join(save_dir, f\"{model_name}_roc_curve.png\")\n",
    "    plt.savefig(roc_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _____________________________Training Part______________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "import gc\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# CBAM module (from paper, simplified version)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        out = out * self.spatial_attention(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Full model with EfficientNetB0 + CBAM\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "class EfficientNet_CBAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNet_CBAM, self).__init__()\n",
    "\n",
    "        # Load pretrained EfficientNet-B0\n",
    "        efficientnet = efficientnet_b0(pretrained=True)\n",
    "\n",
    "        # Keep everything except classifier\n",
    "        self.features = efficientnet.features\n",
    "        self.avgpool = efficientnet.avgpool\n",
    "\n",
    "        # Add CBAM (on final EfficientNet features)\n",
    "        self.cbam = CBAM(in_planes=1280)  # 1280 is output of EfficientNetB0 features\n",
    "\n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.cbam(x)  # Apply CBAM\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "efficientnet_cbam = EfficientNet_CBAM()\n",
    "\n",
    "\n",
    "# ------------------------ Training ------------------------\n",
    "save_path = \"efficientnet_cbam.pth\"\n",
    "log_dir = \"./logs/efficientnet\"\n",
    "\n",
    "train_model(\n",
    "    model=efficientnet_cbam,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=GLOBAL_EPOCH,\n",
    "    lr=GLOBAL_Lr,\n",
    "    wd=1e-4,\n",
    "    log_dir=log_dir,\n",
    "    save_path=save_path,\n",
    "    patience=5,                 # Early stopping patience\n",
    "    use_scheduler=True,         # Enable ReduceLROnPlateau\n",
    "    use_mixed_precision=True    # AMP for faster training\n",
    ")\n",
    "\n",
    "# ------------------------ Load the Best Model ------------------------\n",
    "efficientnet_cbam.load_state_dict(torch.load(save_path))  # Load the best checkpoint before testing\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "evaluate_model(\n",
    "    model=efficientnet,\n",
    "    test_loader=test_loader,\n",
    "    criterion=nn.BCEWithLogitsLoss(),  # Important: BCEWithLogitsLoss\n",
    "    model_name=\"efficientnet_cbam\",\n",
    "    output_file=\"evaluation_results.txt\",  # File to save evaluation text results\n",
    "    save_dir=\"./results/efficientnet\"      # Folder where ROC, CM, and text results will be saved\n",
    ")\n",
    "\n",
    "# ------------------------ Cleanup ------------------------\n",
    "import gc\n",
    "del efficientnet_cbam\n",
    "torch.cuda.empty_cache()  # Clear the GPU memory\n",
    "gc.collect()              # Garbage collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51db76",
   "metadata": {},
   "source": [
    "#  ConvNeXt + Transformer head (modern architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d5f34",
   "metadata": {},
   "source": [
    "https://download.pytorch.org/models/convnext_tiny-983f1562.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e8ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from timm) (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from timm) (0.21.0+cu126)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from timm) (6.0.2)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from torch->timm) (3.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\torch_on\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Installing collected packages: safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.32.4 safetensors-0.5.3 timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f33e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to dataset folders\n",
    "ddiabetic_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\"\n",
    "normal_folder = r\"D:\\Data\\NYC\\Retina\\Data\\Diabetic\\Normal\"\n",
    "\n",
    "# Transformations for training data (with augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Transformations for validation and test data (without augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Combine both folders into a single dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=r\"D:\\Data\\NYC\\Retina\\Data\"\n",
    ")\n",
    "\n",
    "# Explicitly verify class names and indices\n",
    "print(f\"Class to index mapping: {dataset.class_to_idx}\")\n",
    "\n",
    "# Splitting indices for train, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    list(range(len(dataset))),\n",
    "    test_size=0.3,  # 30% for validation and test\n",
    "    stratify=dataset.targets,  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,  # Split remaining 30% equally into validation and test\n",
    "    stratify=[dataset.targets[i] for i in temp_idx],  # Maintain class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create subsets for DataLoader\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "# Apply transformations to subsets\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Verify dataset sizes\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "print(f\"Test images: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "GLOBAL_EPOCH = 250          # or whatever value you want\n",
    "GLOBAL_Lr = 5e-4           # or any learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eva_type = \"full\"          # (example) naming the evaluation file\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # For logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr=0.001, wd=1e-4,\n",
    "                log_dir=\"./logs\", save_path=\"best_model.pth\", patience=5,\n",
    "                use_scheduler=True, use_mixed_precision=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    scaler = amp.GradScaler() if use_mixed_precision else None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with amp.autocast(enabled=use_mixed_precision):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            if use_mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.float().to(device).unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "\n",
    "        # Save best model weights\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Validation loss improved. Model weights saved!\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement in validation loss for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs! Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "    # Plotting\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # <-- For saving CSV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tqdm.auto import tqdm  # <-- Smoother tqdm for all environments (optional but better)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, model_name,\n",
    "                   checkpoint_path=None, output_file=\"evaluation_results.txt\",\n",
    "                   save_dir=\"./results\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # If checkpoint is provided, load it\n",
    "    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}...\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "            if labels.ndim == 1:\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Handle output if model outputs a dict (like ViT sometimes)\n",
    "            if isinstance(outputs, dict) and \"logits\" in outputs:\n",
    "                outputs = outputs[\"logits\"]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    # Explicit class names + class order handling\n",
    "    class_names = ['Diabetic', 'Normal']\n",
    "    class_labels = [0, 1]\n",
    "\n",
    "    # Ensure labels and preds are flattened properly\n",
    "    all_labels = [int(x[0]) for x in all_labels]  # Flatten nested arrays\n",
    "    all_preds = [int(x[0]) for x in all_preds]\n",
    "\n",
    "    # Classification report (handle missing classes safely)\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Confusion matrix (explicit label order)\n",
    "    cm = confusion_matrix(\n",
    "        all_labels, all_preds,\n",
    "        labels=class_labels\n",
    "    )\n",
    "\n",
    "    # Save text results\n",
    "    output_path = os.path.join(save_dir, output_file)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        results = (\n",
    "            f\"Model: {model_name}\\n\"\n",
    "            f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "            f\"Test Accuracy: {test_accuracy:.4f}\\n\"\n",
    "            f\"Classification Report:\\n{report}\\n\"\n",
    "            f\"Confusion Matrix:\\n{cm}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        print(results)\n",
    "        f.write(results)\n",
    "\n",
    "    # -------- Confusion Matrix Plot --------\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # -------- ROC Curve --------\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"{model_name} ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    roc_save_path = os.path.join(save_dir, f\"{model_name}_roc_curve.png\")\n",
    "    plt.savefig(roc_save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _____________________________Training Part______________________________________\n",
    "\n",
    "\n",
    "\n",
    "# ConvNeXt + Transformer head\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ConvNeXt_Transformer(nn.Module):\n",
    "    def __init__(self, convnext_variant='convnext_tiny', num_transformer_layers=2, hidden_dim=768, num_heads=8):\n",
    "        super(ConvNeXt_Transformer, self).__init__()\n",
    "\n",
    "        # Load ConvNeXt backbone without automatic pretrained loading (offline safe)\n",
    "        self.convnext = timm.create_model(convnext_variant, pretrained=False, features_only=True)\n",
    "\n",
    "        # MANUALLY load pretrained weights\n",
    "        checkpoint = torch.load(r\"D:\\Data\\NYC\\Retina\\model_weight\\convnext_tiny-983f1562.pth\", map_location='cpu')\n",
    "        self.convnext.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "        # ConvNeXt outputs feature map (e.g. 7x7xhidden_dim)\n",
    "        self.hidden_dim = self.convnext.feature_info.channels()[-1]\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get ConvNeXt feature map  shape (batch, C, H, W)\n",
    "        features = self.convnext(x)[-1]  # Use last stage features\n",
    "        B, C, H, W = features.shape\n",
    "\n",
    "        # Reshape to sequence  (HW, B, C)\n",
    "        features = features.flatten(2).permute(2, 0, 1)\n",
    "\n",
    "        # Transformer encoder\n",
    "        features = self.transformer_encoder(features)\n",
    "\n",
    "        # Global average pooling over tokens\n",
    "        features = features.mean(dim=0)\n",
    "\n",
    "        # Classification head\n",
    "        out = self.fc(features)\n",
    "        return out\n",
    "\n",
    "\n",
    "convnext_transformer = ConvNeXt_Transformer()\n",
    "\n",
    "\n",
    "# ------------------------ Training ------------------------\n",
    "save_path = \"convnext_transformer.pth\"\n",
    "log_dir = \"./logs/efficientnet\"\n",
    "\n",
    "train_model(\n",
    "    model=convnext_transformer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=GLOBAL_EPOCH,\n",
    "    lr=GLOBAL_Lr,\n",
    "    wd=1e-4,\n",
    "    log_dir=log_dir,\n",
    "    save_path=save_path,\n",
    "    patience=5,                 # Early stopping patience\n",
    "    use_scheduler=True,         # Enable ReduceLROnPlateau\n",
    "    use_mixed_precision=True    # AMP for faster training\n",
    ")\n",
    "\n",
    "# ------------------------ Load the Best Model ------------------------\n",
    "\n",
    "convnext_transformer.load_state_dict(torch.load(save_path))  # Load the best checkpoint before testing\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "evaluate_model(\n",
    "    model=convnext_transformer,\n",
    "    test_loader=test_loader,\n",
    "    criterion=nn.BCEWithLogitsLoss(),  # Important: BCEWithLogitsLoss\n",
    "    model_name=\"convnext_transformer\",\n",
    "    output_file=\"evaluation_results.txt\",  # File to save evaluation text results\n",
    "    save_dir=\"./results/efficientnet\"      # Folder where ROC, CM, and text results will be saved\n",
    ")\n",
    "\n",
    "# ------------------------ Cleanup ------------------------\n",
    "import gc\n",
    "del convnext_transformer\n",
    "torch.cuda.empty_cache()  # Clear the GPU memory\n",
    "gc.collect()              # Garbage collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1683db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
